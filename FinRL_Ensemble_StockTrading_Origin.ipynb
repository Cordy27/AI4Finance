{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXaoZs2lh1hi"
   },
   "source": [
    "# Deep Reinforcement Learning for Stock Trading from Scratch: Multiple Stock Trading Using Ensemble Strategy\n",
    "\n",
    "Tutorials to use OpenAI DRL to trade multiple stocks using ensemble strategy in one Jupyter Notebook | Presented at ICAIF 2020\n",
    "\n",
    "* This notebook is the reimplementation of our paper: Deep Reinforcement Learning for Automated Stock Trading: An Ensemble Strategy, using FinRL.\n",
    "* Check out medium blog for detailed explanations: https://medium.com/@ai4finance/deep-reinforcement-learning-for-automated-stock-trading-f1dad0126a02\n",
    "* Please report any issues to our Github: https://github.com/AI4Finance-LLC/FinRL-Library/issues\n",
    "* **Pytorch Version**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGunVt8oLCVS"
   },
   "source": [
    "# Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HOzAKQ-SLGX6"
   },
   "source": [
    "* [1. Problem Definition](#0)\n",
    "* [2. Getting Started - Load Python packages](#1)\n",
    "    * [2.1. Install Packages](#1.1)    \n",
    "    * [2.2. Check Additional Packages](#1.2)\n",
    "    * [2.3. Import Packages](#1.3)\n",
    "    * [2.4. Create Folders](#1.4)\n",
    "* [3. Download Data](#2)\n",
    "* [4. Preprocess Data](#3)        \n",
    "    * [4.1. Technical Indicators](#3.1)\n",
    "    * [4.2. Perform Feature Engineering](#3.2)\n",
    "* [5.Build Environment](#4)  \n",
    "    * [5.1. Training & Trade Data Split](#4.1)\n",
    "    * [5.2. User-defined Environment](#4.2)   \n",
    "    * [5.3. Initialize Environment](#4.3)    \n",
    "* [6.Implement DRL Algorithms](#5)  \n",
    "* [7.Backtesting Performance](#6)  \n",
    "    * [7.1. BackTestStats](#6.1)\n",
    "    * [7.2. BackTestPlot](#6.2)   \n",
    "    * [7.3. Baseline Stats](#6.3)   \n",
    "    * [7.3. Compare to Stock Market Index](#6.4)             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sApkDlD9LIZv"
   },
   "source": [
    "<a id='0'></a>\n",
    "# Part 1. Problem Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjLD2TZSLKZ-"
   },
   "source": [
    "This problem is to design an automated trading solution for single stock trading. We model the stock trading process as a Markov Decision Process (MDP). We then formulate our trading goal as a maximization problem.\n",
    "\n",
    "The algorithm is trained using Deep Reinforcement Learning (DRL) algorithms and the components of the reinforcement learning environment are:\n",
    "\n",
    "\n",
    "* Action: The action space describes the allowed actions that the agent interacts with the\n",
    "environment. Normally, a ∈ A includes three actions: a ∈ {−1, 0, 1}, where −1, 0, 1 represent\n",
    "selling, holding, and buying one stock. Also, an action can be carried upon multiple shares. We use\n",
    "an action space {−k, ..., −1, 0, 1, ..., k}, where k denotes the number of shares. For example, \"Buy\n",
    "10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or −10, respectively\n",
    "\n",
    "* Reward function: r(s, a, s′) is the incentive mechanism for an agent to learn a better action. The change of the portfolio value when action a is taken at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio\n",
    "values at state s′ and s, respectively\n",
    "\n",
    "* State: The state space describes the observations that the agent receives from the environment. Just as a human trader needs to analyze various information before executing a trade, so\n",
    "our trading agent observes many different features to better learn in an interactive environment.\n",
    "\n",
    "* Environment: Dow 30 consituents\n",
    "\n",
    "\n",
    "The data of the single stock that we will be using for this case study is obtained from Yahoo Finance API. The data contains Open-High-Low-Close price and volume.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ffsre789LY08"
   },
   "source": [
    "<a id='1'></a>\n",
    "# Part 2. Getting Started- Load Python Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uy5_PTmOh1hj"
   },
   "source": [
    "<a id='1.1'></a>\n",
    "## 2.1. Install all the packages through FinRL library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-09T13:25:57.926560Z",
     "iopub.status.busy": "2025-05-09T13:25:57.926219Z",
     "iopub.status.idle": "2025-05-09T13:26:44.744976Z",
     "shell.execute_reply": "2025-05-09T13:26:44.744062Z",
     "shell.execute_reply.started": "2025-05-09T13:25:57.926538Z"
    },
    "id": "mPT0ipYE28wL",
    "outputId": "5108c559-607e-4546-e94b-38ba34d475da",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.11/site-packages (25.1.1)\n",
      "\u001b[33mWARNING: Error parsing dependencies of omegaconf: .* suffix can only be used with `==` or `!=` operators\n",
      "    PyYAML (>=5.1.*)\n",
      "            ~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Error parsing dependencies of pytorch-lightning: .* suffix can only be used with `==` or `!=` operators\n",
      "    torch (>=1.9.*)\n",
      "           ~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: wrds in /usr/local/lib/python3.11/site-packages (3.3.0)\n",
      "Requirement already satisfied: packaging<=24.2 in /usr/local/lib/python3.11/site-packages (from wrds) (24.2)\n",
      "Requirement already satisfied: pandas<2.3,>=2.2 in /usr/local/lib/python3.11/site-packages (from wrds) (2.2.3)\n",
      "Requirement already satisfied: psycopg2-binary<2.10,>=2.9 in /usr/local/lib/python3.11/site-packages (from wrds) (2.9.10)\n",
      "Requirement already satisfied: sqlalchemy<2.1,>=2 in /usr/local/lib/python3.11/site-packages (from wrds) (2.0.40)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/site-packages (from pandas<2.3,>=2.2->wrds) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/site-packages (from pandas<2.3,>=2.2->wrds) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/site-packages (from pandas<2.3,>=2.2->wrds) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/site-packages (from pandas<2.3,>=2.2->wrds) (2025.2)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/site-packages (from sqlalchemy<2.1,>=2->wrds) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/site-packages (from sqlalchemy<2.1,>=2->wrds) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas<2.3,>=2.2->wrds) (1.17.0)\n",
      "\u001b[33mWARNING: Error parsing dependencies of omegaconf: .* suffix can only be used with `==` or `!=` operators\n",
      "    PyYAML (>=5.1.*)\n",
      "            ~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Error parsing dependencies of pytorch-lightning: .* suffix can only be used with `==` or `!=` operators\n",
      "    torch (>=1.9.*)\n",
      "           ~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: swig in /usr/local/lib/python3.11/site-packages (4.3.1)\n",
      "\u001b[33mWARNING: Error parsing dependencies of omegaconf: .* suffix can only be used with `==` or `!=` operators\n",
      "    PyYAML (>=5.1.*)\n",
      "            ~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Error parsing dependencies of pytorch-lightning: .* suffix can only be used with `==` or `!=` operators\n",
      "    torch (>=1.9.*)\n",
      "           ~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Collecting git+https://github.com/AI4Finance-Foundation/FinRL.git\n",
      "  Cloning https://github.com/AI4Finance-Foundation/FinRL.git to /tmp/pip-req-build-ff675z8r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/FinRL.git /tmp/pip-req-build-ff675z8r\n",
      "  Resolved https://github.com/AI4Finance-Foundation/FinRL.git to commit 9175621d1235a335a667ee4796d7722e555a9e78\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: alpaca-py<0.38,>=0.37 in /usr/local/lib/python3.11/site-packages (from finrl==0.3.8) (0.37.0)\n",
      "Requirement already satisfied: alpaca-trade-api<4,>=3 in /usr/local/lib/python3.11/site-packages (from finrl==0.3.8) (3.2.0)\n",
      "Requirement already satisfied: ccxt<4,>=3 in /usr/local/lib/python3.11/site-packages (from finrl==0.3.8) (3.1.60)\n",
      "Collecting elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git (from finrl==0.3.8)\n",
      "  Cloning https://github.com/AI4Finance-Foundation/ElegantRL.git to /tmp/pip-install-efdck7c2/elegantrl_f0c1294f479247a8933d23b69691e4fc\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/ElegantRL.git /tmp/pip-install-efdck7c2/elegantrl_f0c1294f479247a8933d23b69691e4fc\n",
      "  Resolved https://github.com/AI4Finance-Foundation/ElegantRL.git to commit 8ea76afc3e7f1564ae9f0e69e70254116d575fe9\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: jqdatasdk<2,>=1 in /usr/local/lib/python3.11/site-packages (from finrl==0.3.8) (1.9.7)\n",
      "Requirement already satisfied: pyfolio-reloaded<0.10,>=0.9 in /usr/local/lib/python3.11/site-packages (from finrl==0.3.8) (0.9.8)\n",
      "Requirement already satisfied: pyportfolioopt<2,>=1 in /usr/local/lib/python3.11/site-packages (from finrl==0.3.8) (1.5.6)\n",
      "Requirement already satisfied: ray<3,>=2 in /usr/local/lib/python3.11/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (2.44.1)\n",
      "Requirement already satisfied: scikit-learn<2,>=1 in /usr/local/lib/python3.11/site-packages (from finrl==0.3.8) (1.6.1)\n",
      "Requirement already satisfied: selenium<5,>=4 in /usr/local/lib/python3.11/site-packages (from finrl==0.3.8) (4.32.0)\n",
      "Requirement already satisfied: stable-baselines3>=2.0.0a5 in /usr/local/lib/python3.11/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.6.0)\n",
      "Requirement already satisfied: stockstats<0.6,>=0.5 in /usr/local/lib/python3.11/site-packages (from finrl==0.3.8) (0.5.4)\n",
      "Requirement already satisfied: webdriver-manager<5,>=4 in /usr/local/lib/python3.11/site-packages (from finrl==0.3.8) (4.0.2)\n",
      "Requirement already satisfied: wrds<4,>=3 in /usr/local/lib/python3.11/site-packages (from finrl==0.3.8) (3.3.0)\n",
      "Requirement already satisfied: yfinance<0.3,>=0.2 in /usr/local/lib/python3.11/site-packages (from finrl==0.3.8) (0.2.58)\n",
      "Requirement already satisfied: th in /usr/local/lib/python3.11/site-packages (from elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (0.4.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/site-packages (from elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (1.26.4)\n",
      "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/site-packages (from elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (1.1.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/site-packages (from elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (3.10.1)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.3 in /usr/local/lib/python3.11/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (1.0.3)\n",
      "Requirement already satisfied: pandas>=1.5.3 in /usr/local/lib/python3.11/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.2.3)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.3 in /usr/local/lib/python3.11/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.11.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.30.0 in /usr/local/lib/python3.11/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.32.3)\n",
      "Requirement already satisfied: sseclient-py<2.0.0,>=1.7.2 in /usr/local/lib/python3.11/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (1.8.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (10.4)\n",
      "Requirement already satisfied: urllib3<2,>1.24 in /usr/local/lib/python3.11/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (1.26.20)\n",
      "Requirement already satisfied: websocket-client<2,>=0.56.0 in /usr/local/lib/python3.11/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (1.8.0)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.3 in /usr/local/lib/python3.11/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (3.11.16)\n",
      "Requirement already satisfied: PyYAML==6.0.1 in /usr/local/lib/python3.11/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (6.0.1)\n",
      "Requirement already satisfied: deprecation==2.1.0 in /usr/local/lib/python3.11/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (2.1.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/site-packages (from deprecation==2.1.0->alpaca-trade-api<4,>=3->finrl==0.3.8) (24.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (6.4.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (1.19.0)\n",
      "Requirement already satisfied: setuptools>=60.9.0 in /usr/local/lib/python3.11/site-packages (from ccxt<4,>=3->finrl==0.3.8) (69.5.1)\n",
      "Requirement already satisfied: certifi>=2018.1.18 in /usr/local/lib/python3.11/site-packages (from ccxt<4,>=3->finrl==0.3.8) (2025.1.31)\n",
      "Requirement already satisfied: cryptography>=2.6.1 in /usr/local/lib/python3.11/site-packages (from ccxt<4,>=3->finrl==0.3.8) (44.0.2)\n",
      "Requirement already satisfied: aiodns>=1.1.1 in /usr/local/lib/python3.11/site-packages (from ccxt<4,>=3->finrl==0.3.8) (3.4.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.11/site-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (1.17.0)\n",
      "Requirement already satisfied: SQLAlchemy>=1.2.8 in /usr/local/lib/python3.11/site-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (2.0.40)\n",
      "Requirement already satisfied: pymysql>=0.7.6 in /usr/local/lib/python3.11/site-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (1.1.1)\n",
      "Requirement already satisfied: thriftpy2!=0.5.1,>=0.3.9 in /usr/local/lib/python3.11/site-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (0.5.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.3->alpaca-py<0.38,>=0.37->finrl==0.3.8) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.3->alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.33.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.3->alpaca-py<0.38,>=0.37->finrl==0.3.8) (4.12.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.3->alpaca-py<0.38,>=0.37->finrl==0.3.8) (0.4.0)\n",
      "Requirement already satisfied: ipython>=3.2.3 in /usr/local/lib/python3.11/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (9.0.2)\n",
      "Requirement already satisfied: pytz>=2014.10 in /usr/local/lib/python3.11/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (2025.2)\n",
      "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.11/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (1.12.0)\n",
      "Requirement already satisfied: seaborn>=0.7.1 in /usr/local/lib/python3.11/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.13.2)\n",
      "Requirement already satisfied: empyrical-reloaded>=0.5.9 in /usr/local/lib/python3.11/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.5.11)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/site-packages (from pandas>=1.5.3->alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/site-packages (from pandas>=1.5.3->alpaca-py<0.38,>=0.37->finrl==0.3.8) (2025.2)\n",
      "Requirement already satisfied: cvxpy>=1.1.19 in /usr/local/lib/python3.11/site-packages (from pyportfolioopt<2,>=1->finrl==0.3.8) (1.6.5)\n",
      "Requirement already satisfied: ecos<3.0.0,>=2.0.14 in /usr/local/lib/python3.11/site-packages (from pyportfolioopt<2,>=1->finrl==0.3.8) (2.0.14)\n",
      "Requirement already satisfied: plotly<6.0.0,>=5.0.0 in /usr/local/lib/python3.11/site-packages (from pyportfolioopt<2,>=1->finrl==0.3.8) (5.24.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/site-packages (from plotly<6.0.0,>=5.0.0->pyportfolioopt<2,>=1->finrl==0.3.8) (9.1.2)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (8.1.8)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (3.17.0)\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (4.23.0)\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.11/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (3.20.3)\n",
      "Requirement already satisfied: aiohttp_cors in /usr/local/lib/python3.11/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.8.1)\n",
      "Requirement already satisfied: colorful in /usr/local/lib/python3.11/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.5.6)\n",
      "Requirement already satisfied: py-spy>=0.2.0 in /usr/local/lib/python3.11/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.4.0)\n",
      "Requirement already satisfied: grpcio>=1.42.0 in /usr/local/lib/python3.11/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (1.71.0)\n",
      "Requirement already satisfied: opencensus in /usr/local/lib/python3.11/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.11.4)\n",
      "Requirement already satisfied: prometheus_client>=0.7.1 in /usr/local/lib/python3.11/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.21.1)\n",
      "Requirement already satisfied: smart_open in /usr/local/lib/python3.11/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (6.4.0)\n",
      "Requirement already satisfied: virtualenv!=20.21.1,>=20.0.24 in /usr/local/lib/python3.11/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (20.30.0)\n",
      "Requirement already satisfied: tensorboardX>=1.9 in /usr/local/lib/python3.11/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (2.6.2.2)\n",
      "Requirement already satisfied: pyarrow>=9.0.0 in /usr/local/lib/python3.11/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (19.0.1)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (2024.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests<3.0.0,>=2.30.0->alpaca-py<0.38,>=0.37->finrl==0.3.8) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests<3.0.0,>=2.30.0->alpaca-py<0.38,>=0.37->finrl==0.3.8) (3.10)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/site-packages (from scikit-learn<2,>=1->finrl==0.3.8) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/site-packages (from scikit-learn<2,>=1->finrl==0.3.8) (3.6.0)\n",
      "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.11/site-packages (from selenium<5,>=4->finrl==0.3.8) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.11/site-packages (from selenium<5,>=4->finrl==0.3.8) (0.12.2)\n",
      "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/site-packages (from trio~=0.17->selenium<5,>=4->finrl==0.3.8) (2.4.0)\n",
      "Requirement already satisfied: outcome in /usr/local/lib/python3.11/site-packages (from trio~=0.17->selenium<5,>=4->finrl==0.3.8) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/site-packages (from trio~=0.17->selenium<5,>=4->finrl==0.3.8) (1.3.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.11/site-packages (from trio-websocket~=0.9->selenium<5,>=4->finrl==0.3.8) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/site-packages (from urllib3[socks]<3,>=1.26->selenium<5,>=4->finrl==0.3.8) (1.7.1)\n",
      "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/site-packages (from webdriver-manager<5,>=4->finrl==0.3.8) (1.1.0)\n",
      "Requirement already satisfied: psycopg2-binary<2.10,>=2.9 in /usr/local/lib/python3.11/site-packages (from wrds<4,>=3->finrl==0.3.8) (2.9.10)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/site-packages (from SQLAlchemy>=1.2.8->jqdatasdk<2,>=1->finrl==0.3.8) (3.1.1)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (0.0.11)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (4.3.7)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (2.4.6)\n",
      "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (3.17.3)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (4.13.3)\n",
      "Requirement already satisfied: curl_cffi>=0.7 in /usr/local/lib/python3.11/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (0.10.0)\n",
      "Requirement already satisfied: pycares>=4.0.0 in /usr/local/lib/python3.11/site-packages (from aiodns>=1.1.1->ccxt<4,>=3->finrl==0.3.8) (4.8.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/site-packages (from beautifulsoup4>=4.11.1->yfinance<0.3,>=0.2->finrl==0.3.8) (2.6)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/site-packages (from cryptography>=2.6.1->ccxt<4,>=3->finrl==0.3.8) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=2.6.1->ccxt<4,>=3->finrl==0.3.8) (2.22)\n",
      "Requirement already satisfied: osqp>=0.6.2 in /usr/local/lib/python3.11/site-packages (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.8) (1.0.4)\n",
      "Requirement already satisfied: clarabel>=0.5.0 in /usr/local/lib/python3.11/site-packages (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.8) (0.10.0)\n",
      "Requirement already satisfied: scs>=3.2.4.post1 in /usr/local/lib/python3.11/site-packages (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.8) (3.2.7.post2)\n",
      "Requirement already satisfied: bottleneck>=1.3.0 in /usr/local/lib/python3.11/site-packages (from empyrical-reloaded>=0.5.9->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (1.4.2)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.11/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (4.4.2)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /usr/local/lib/python3.11/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.11/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (3.0.50)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.11/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /usr/local/lib/python3.11/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /usr/local/lib/python3.11/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (5.14.3)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/site-packages (from jedi>=0.16->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.8.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/site-packages (from matplotlib->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/site-packages (from matplotlib->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/site-packages (from matplotlib->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/site-packages (from matplotlib->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/site-packages (from matplotlib->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/site-packages (from matplotlib->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/site-packages (from osqp>=0.6.2->cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.8) (3.1.6)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/site-packages (from pexpect>4.3->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.7.0)\n",
      "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.11/site-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.3.1)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/site-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.1.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/site-packages (from gymnasium->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (0.0.4)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (12.8.93)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (4.11.0.86)\n",
      "Requirement already satisfied: pygame in /usr/local/lib/python3.11/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.6.1)\n",
      "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.11/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.16.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (7.0.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (4.67.1)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (13.9.4)\n",
      "Requirement already satisfied: ale-py>=0.9.0 in /usr/local/lib/python3.11/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.11.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.1.3)\n",
      "Requirement already satisfied: Cython>=3.0.10 in /usr/local/lib/python3.11/site-packages (from thriftpy2!=0.5.1,>=0.3.9->jqdatasdk<2,>=1->finrl==0.3.8) (3.1.0)\n",
      "Requirement already satisfied: ply<4.0,>=3.4 in /usr/local/lib/python3.11/site-packages (from thriftpy2!=0.5.1,>=0.3.9->jqdatasdk<2,>=1->finrl==0.3.8) (3.11)\n",
      "Requirement already satisfied: distlib<1,>=0.3.7 in /usr/local/lib/python3.11/site-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<3,>=2->finrl==0.3.8) (0.3.9)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.0.2)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium<5,>=4->finrl==0.3.8) (0.14.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/site-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/site-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/site-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (0.23.1)\n",
      "Requirement already satisfied: opencensus-context>=0.1.3 in /usr/local/lib/python3.11/site-packages (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (0.1.3)\n",
      "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.11/site-packages (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (2.24.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (1.70.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (1.26.1)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (2.40.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/site-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/site-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/site-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.11/site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (0.6.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/site-packages (from rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.1.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.11/site-packages (from stack_data->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.11/site-packages (from stack_data->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.11/site-packages (from stack_data->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.2.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/site-packages (from sympy->torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (1.3.0)\n",
      "Requirement already satisfied: niltype<2.0,>=0.3 in /usr/local/lib/python3.11/site-packages (from th->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (1.0.2)\n",
      "\u001b[33mWARNING: Error parsing dependencies of omegaconf: .* suffix can only be used with `==` or `!=` operators\n",
      "    PyYAML (>=5.1.*)\n",
      "            ~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Error parsing dependencies of pytorch-lightning: .* suffix can only be used with `==` or `!=` operators\n",
      "    torch (>=1.9.*)\n",
      "           ~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# ## install finrl library\n",
    "!pip install --upgrade pip\n",
    "!pip install wrds\n",
    "!pip install swig\n",
    "!apt-get update -y -qq && apt-get install -y -qq cmake libopenmpi-dev python3-dev zlib1g-dev libgl1-mesa-glx swig\n",
    "!pip install git+https://github.com/AI4Finance-Foundation/FinRL.git\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osBHhVysOEzi"
   },
   "source": [
    "\n",
    "<a id='1.2'></a>\n",
    "## 2.2. Check if the additional packages needed are present, if not install them.\n",
    "* Yahoo Finance API\n",
    "* pandas\n",
    "* numpy\n",
    "* matplotlib\n",
    "* stockstats\n",
    "* OpenAI gym\n",
    "* stable-baselines\n",
    "* tensorflow\n",
    "* pyfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGv01K8Sh1hn"
   },
   "source": [
    "<a id='1.3'></a>\n",
    "## 2.3. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T13:26:44.747653Z",
     "iopub.status.busy": "2025-05-09T13:26:44.747134Z",
     "iopub.status.idle": "2025-05-09T13:26:44.752301Z",
     "shell.execute_reply": "2025-05-09T13:26:44.751707Z",
     "shell.execute_reply.started": "2025-05-09T13:26:44.747604Z"
    },
    "id": "EeMK7Uentj1V",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T13:26:44.753502Z",
     "iopub.status.busy": "2025-05-09T13:26:44.753211Z",
     "iopub.status.idle": "2025-05-09T13:26:46.470593Z",
     "shell.execute_reply": "2025-05-09T13:26:46.469648Z",
     "shell.execute_reply.started": "2025-05-09T13:26:44.753481Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: pandas_market_calendars in /usr/local/lib/python3.11/site-packages (5.1.0)\n",
      "Requirement already satisfied: pandas>=1.1 in /usr/local/lib/python3.11/site-packages (from pandas_market_calendars) (2.2.3)\n",
      "Requirement already satisfied: tzdata in /usr/local/lib/python3.11/site-packages (from pandas_market_calendars) (2025.2)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/site-packages (from pandas_market_calendars) (2.9.0.post0)\n",
      "Requirement already satisfied: exchange-calendars>=3.3 in /usr/local/lib/python3.11/site-packages (from pandas_market_calendars) (4.10)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (1.26.4)\n",
      "Requirement already satisfied: pyluach in /usr/local/lib/python3.11/site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (2.2.0)\n",
      "Requirement already satisfied: toolz in /usr/local/lib/python3.11/site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (1.0.0)\n",
      "Requirement already satisfied: korean_lunar_calendar in /usr/local/lib/python3.11/site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (0.3.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/site-packages (from pandas>=1.1->pandas_market_calendars) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/site-packages (from python-dateutil->pandas_market_calendars) (1.17.0)\n",
      "\u001b[33mWARNING: Error parsing dependencies of omegaconf: .* suffix can only be used with `==` or `!=` operators\n",
      "    PyYAML (>=5.1.*)\n",
      "            ~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Error parsing dependencies of pytorch-lightning: .* suffix can only be used with `==` or `!=` operators\n",
      "    torch (>=1.9.*)\n",
      "           ~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pandas_market_calendars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T13:26:46.472151Z",
     "iopub.status.busy": "2025-05-09T13:26:46.471779Z",
     "iopub.status.idle": "2025-05-09T13:26:53.536479Z",
     "shell.execute_reply": "2025-05-09T13:26:53.535771Z",
     "shell.execute_reply.started": "2025-05-09T13:26:46.472118Z"
    },
    "id": "lPqeTTwoh1hn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 21:26:49.540176: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-09 21:26:50.844212: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlib.use('Agg')\n",
    "import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "from finrl.config_tickers import DOW_30_TICKER\n",
    "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent,DRLEnsembleAgent\n",
    "from finrl.plot import backtest_stats, backtest_plot, get_daily_return, get_baseline\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../FinRL-Library\")\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2owTj985RW4"
   },
   "source": [
    "<a id='1.4'></a>\n",
    "## 2.4. Create Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T13:26:53.538835Z",
     "iopub.status.busy": "2025-05-09T13:26:53.538303Z",
     "iopub.status.idle": "2025-05-09T13:26:53.543473Z",
     "shell.execute_reply": "2025-05-09T13:26:53.542712Z",
     "shell.execute_reply.started": "2025-05-09T13:26:53.538810Z"
    },
    "id": "w9A8CN5R5PuZ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from finrl.main import check_and_make_directories\n",
    "from finrl.config import (\n",
    "    DATA_SAVE_DIR,\n",
    "    TRAINED_MODEL_DIR,\n",
    "    TENSORBOARD_LOG_DIR,\n",
    "    RESULTS_DIR,\n",
    "    INDICATORS,\n",
    "    TRAIN_START_DATE,\n",
    "    TRAIN_END_DATE,\n",
    "    TEST_START_DATE,\n",
    "    TEST_END_DATE,\n",
    "    TRADE_START_DATE,\n",
    "    TRADE_END_DATE,\n",
    ")\n",
    "\n",
    "check_and_make_directories([DATA_SAVE_DIR, TRAINED_MODEL_DIR, TENSORBOARD_LOG_DIR, RESULTS_DIR])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A289rQWMh1hq"
   },
   "source": [
    "<a id='2'></a>\n",
    "# Part 3. Download Data\n",
    "Yahoo Finance is a website that provides stock data, financial news, financial reports, etc. All the data provided by Yahoo Finance is free.\n",
    "* FinRL uses a class **YahooDownloader** to fetch data from Yahoo Finance API\n",
    "* Call Limit: Using the Public API (without authentication), you are limited to 2,000 requests per hour per IP (or up to a total of 48,000 requests a day).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPeQ7iS-LoMm"
   },
   "source": [
    "\n",
    "\n",
    "-----\n",
    "class YahooDownloader:\n",
    "    Provides methods for retrieving daily stock data from\n",
    "    Yahoo Finance API\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "        start_date : str\n",
    "            start date of the data (modified from config.py)\n",
    "        end_date : str\n",
    "            end date of the data (modified from config.py)\n",
    "        ticker_list : list\n",
    "            a list of stock tickers (modified from config.py)\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    fetch_data()\n",
    "        Fetches data from yahoo API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-09T13:26:53.544458Z",
     "iopub.status.busy": "2025-05-09T13:26:53.544173Z",
     "iopub.status.idle": "2025-05-09T13:26:53.547911Z",
     "shell.execute_reply": "2025-05-09T13:26:53.547292Z",
     "shell.execute_reply.started": "2025-05-09T13:26:53.544437Z"
    },
    "id": "JzqRRTOX6aFu",
    "outputId": "8b6edbae-a05b-4ef6-dc8f-2e3c073c1e1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AXP', 'AMGN', 'AAPL', 'BA', 'CAT', 'CSCO', 'CVX', 'GS', 'HD', 'HON', 'IBM', 'INTC', 'JNJ', 'KO', 'JPM', 'MCD', 'MMM', 'MRK', 'MSFT', 'NKE', 'PG', 'TRV', 'UNH', 'CRM', 'VZ', 'V', 'WBA', 'WMT', 'DIS', 'DOW']\n"
     ]
    }
   ],
   "source": [
    "print(DOW_30_TICKER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-09T13:26:53.548856Z",
     "iopub.status.busy": "2025-05-09T13:26:53.548610Z",
     "iopub.status.idle": "2025-05-09T13:26:53.680490Z",
     "shell.execute_reply": "2025-05-09T13:26:53.679743Z",
     "shell.execute_reply.started": "2025-05-09T13:26:53.548837Z"
    },
    "id": "yCKm4om-s9kE",
    "outputId": "ecf97b69-5365-486b-8367-e552110b0064",
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_START_DATE = '2009-01-01'\n",
    "TRAIN_END_DATE = '2015-09-30'\n",
    "TEST_START_DATE = '2015-10-01'\n",
    "TEST_END_DATE = '2020-05-08'\n",
    "\n",
    "# TRAIN_START_DATE = '2010-01-01'\n",
    "# TRAIN_END_DATE = '2021-10-01'\n",
    "# TEST_START_DATE = '2021-10-01'\n",
    "# TEST_END_DATE = '2023-03-01'\n",
    "\n",
    "df = pd.read_csv('dow_30.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "execution": {
     "iopub.execute_input": "2025-05-09T13:26:53.681580Z",
     "iopub.status.busy": "2025-05-09T13:26:53.681285Z",
     "iopub.status.idle": "2025-05-09T13:26:53.693513Z",
     "shell.execute_reply": "2025-05-09T13:26:53.692910Z",
     "shell.execute_reply.started": "2025-05-09T13:26:53.681558Z"
    },
    "id": "GiRuFOTOtj1Y",
    "outputId": "53ff7d29-7a5d-47e7-9a59-61f0bcf9df93"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>volume</th>\n",
       "      <th>tic</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>14.929291</td>\n",
       "      <td>15.076035</td>\n",
       "      <td>14.211016</td>\n",
       "      <td>14.342314</td>\n",
       "      <td>10955700</td>\n",
       "      <td>AXP</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-01-05</td>\n",
       "      <td>15.408139</td>\n",
       "      <td>15.632116</td>\n",
       "      <td>14.674418</td>\n",
       "      <td>14.828886</td>\n",
       "      <td>16019200</td>\n",
       "      <td>AXP</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-01-06</td>\n",
       "      <td>16.273165</td>\n",
       "      <td>16.512589</td>\n",
       "      <td>15.454487</td>\n",
       "      <td>15.678464</td>\n",
       "      <td>13820200</td>\n",
       "      <td>AXP</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-01-07</td>\n",
       "      <td>15.587648</td>\n",
       "      <td>16.140733</td>\n",
       "      <td>15.447429</td>\n",
       "      <td>15.992725</td>\n",
       "      <td>15699900</td>\n",
       "      <td>AXP</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-01-08</td>\n",
       "      <td>15.611012</td>\n",
       "      <td>15.712280</td>\n",
       "      <td>15.112455</td>\n",
       "      <td>15.424052</td>\n",
       "      <td>12255100</td>\n",
       "      <td>AXP</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date      close       high        low       open    volume  tic  day\n",
       "0  2009-01-02  14.929291  15.076035  14.211016  14.342314  10955700  AXP    5\n",
       "1  2009-01-05  15.408139  15.632116  14.674418  14.828886  16019200  AXP    1\n",
       "2  2009-01-06  16.273165  16.512589  15.454487  15.678464  13820200  AXP    2\n",
       "3  2009-01-07  15.587648  16.140733  15.447429  15.992725  15699900  AXP    3\n",
       "4  2009-01-08  15.611012  15.712280  15.112455  15.424052  12255100  AXP    4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "execution": {
     "iopub.execute_input": "2025-05-09T13:26:53.694502Z",
     "iopub.status.busy": "2025-05-09T13:26:53.694241Z",
     "iopub.status.idle": "2025-05-09T13:26:53.703074Z",
     "shell.execute_reply": "2025-05-09T13:26:53.702505Z",
     "shell.execute_reply.started": "2025-05-09T13:26:53.694483Z"
    },
    "id": "DSw4ZEzVtj1Z",
    "outputId": "b45c7da3-2e52-4484-c8ce-ff8b1f50c43e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>volume</th>\n",
       "      <th>tic</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>83106</th>\n",
       "      <td>2020-05-01</td>\n",
       "      <td>26.061184</td>\n",
       "      <td>26.952164</td>\n",
       "      <td>25.761631</td>\n",
       "      <td>26.675653</td>\n",
       "      <td>8709300</td>\n",
       "      <td>DOW</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83107</th>\n",
       "      <td>2020-05-04</td>\n",
       "      <td>25.392944</td>\n",
       "      <td>25.454390</td>\n",
       "      <td>24.394431</td>\n",
       "      <td>24.655579</td>\n",
       "      <td>6248300</td>\n",
       "      <td>DOW</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83108</th>\n",
       "      <td>2020-05-05</td>\n",
       "      <td>25.631058</td>\n",
       "      <td>26.537400</td>\n",
       "      <td>25.523526</td>\n",
       "      <td>25.830762</td>\n",
       "      <td>5041600</td>\n",
       "      <td>DOW</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83109</th>\n",
       "      <td>2020-05-06</td>\n",
       "      <td>24.548054</td>\n",
       "      <td>25.634897</td>\n",
       "      <td>24.486607</td>\n",
       "      <td>25.124119</td>\n",
       "      <td>4601500</td>\n",
       "      <td>DOW</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83110</th>\n",
       "      <td>2020-05-07</td>\n",
       "      <td>25.231651</td>\n",
       "      <td>25.961334</td>\n",
       "      <td>24.970502</td>\n",
       "      <td>24.985864</td>\n",
       "      <td>5056200</td>\n",
       "      <td>DOW</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             date      close       high        low       open   volume  tic  \\\n",
       "83106  2020-05-01  26.061184  26.952164  25.761631  26.675653  8709300  DOW   \n",
       "83107  2020-05-04  25.392944  25.454390  24.394431  24.655579  6248300  DOW   \n",
       "83108  2020-05-05  25.631058  26.537400  25.523526  25.830762  5041600  DOW   \n",
       "83109  2020-05-06  24.548054  25.634897  24.486607  25.124119  4601500  DOW   \n",
       "83110  2020-05-07  25.231651  25.961334  24.970502  24.985864  5056200  DOW   \n",
       "\n",
       "       day  \n",
       "83106    5  \n",
       "83107    1  \n",
       "83108    2  \n",
       "83109    3  \n",
       "83110    4  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-09T13:26:53.704040Z",
     "iopub.status.busy": "2025-05-09T13:26:53.703769Z",
     "iopub.status.idle": "2025-05-09T13:26:53.707841Z",
     "shell.execute_reply": "2025-05-09T13:26:53.707223Z",
     "shell.execute_reply.started": "2025-05-09T13:26:53.704020Z"
    },
    "id": "CV3HrZHLh1hy",
    "outputId": "a627cbd2-4379-42a1-8138-f4c11f43f957"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(83111, 8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "execution": {
     "iopub.execute_input": "2025-05-09T13:26:53.708841Z",
     "iopub.status.busy": "2025-05-09T13:26:53.708592Z",
     "iopub.status.idle": "2025-05-09T13:26:53.730601Z",
     "shell.execute_reply": "2025-05-09T13:26:53.730048Z",
     "shell.execute_reply.started": "2025-05-09T13:26:53.708822Z"
    },
    "id": "4hYkeaPiICHS",
    "outputId": "fe9e0d63-1d7b-4b66-eaa3-aaf3005c757f",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>volume</th>\n",
       "      <th>tic</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5712</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>2.730993</td>\n",
       "      <td>2.739721</td>\n",
       "      <td>2.562771</td>\n",
       "      <td>2.584438</td>\n",
       "      <td>746015200</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2856</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>41.151958</td>\n",
       "      <td>41.214743</td>\n",
       "      <td>40.286922</td>\n",
       "      <td>40.872914</td>\n",
       "      <td>6547900</td>\n",
       "      <td>AMGN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>14.929291</td>\n",
       "      <td>15.076035</td>\n",
       "      <td>14.211016</td>\n",
       "      <td>14.342314</td>\n",
       "      <td>10955700</td>\n",
       "      <td>AXP</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8568</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>33.941093</td>\n",
       "      <td>34.173619</td>\n",
       "      <td>32.088396</td>\n",
       "      <td>32.103398</td>\n",
       "      <td>7010200</td>\n",
       "      <td>BA</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11424</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>30.344687</td>\n",
       "      <td>30.389967</td>\n",
       "      <td>28.921571</td>\n",
       "      <td>29.050946</td>\n",
       "      <td>7117200</td>\n",
       "      <td>CAT</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             date      close       high        low       open     volume  \\\n",
       "5712   2009-01-02   2.730993   2.739721   2.562771   2.584438  746015200   \n",
       "2856   2009-01-02  41.151958  41.214743  40.286922  40.872914    6547900   \n",
       "0      2009-01-02  14.929291  15.076035  14.211016  14.342314   10955700   \n",
       "8568   2009-01-02  33.941093  34.173619  32.088396  32.103398    7010200   \n",
       "11424  2009-01-02  30.344687  30.389967  28.921571  29.050946    7117200   \n",
       "\n",
       "        tic  day  \n",
       "5712   AAPL    5  \n",
       "2856   AMGN    5  \n",
       "0       AXP    5  \n",
       "8568     BA    5  \n",
       "11424   CAT    5  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(['date','tic']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-09T13:26:53.731561Z",
     "iopub.status.busy": "2025-05-09T13:26:53.731309Z",
     "iopub.status.idle": "2025-05-09T13:26:53.738355Z",
     "shell.execute_reply": "2025-05-09T13:26:53.737700Z",
     "shell.execute_reply.started": "2025-05-09T13:26:53.731542Z"
    },
    "id": "a2vryMsdNL9H",
    "outputId": "9b64b53f-2755-4ad0-9f3a-e8c157d1dad3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.tic.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2025-05-09T13:26:53.739302Z",
     "iopub.status.busy": "2025-05-09T13:26:53.739043Z",
     "iopub.status.idle": "2025-05-09T13:26:53.747098Z",
     "shell.execute_reply": "2025-05-09T13:26:53.746547Z",
     "shell.execute_reply.started": "2025-05-09T13:26:53.739284Z"
    },
    "id": "XcNyXa7RNPrF",
    "outputId": "b3911ec3-c6ab-444e-88e4-9429e03df49f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tic\n",
       "AXP     2856\n",
       "AMGN    2856\n",
       "AAPL    2856\n",
       "BA      2856\n",
       "CAT     2856\n",
       "CSCO    2856\n",
       "CVX     2856\n",
       "GS      2856\n",
       "HD      2856\n",
       "HON     2856\n",
       "IBM     2856\n",
       "INTC    2856\n",
       "JNJ     2856\n",
       "KO      2856\n",
       "JPM     2856\n",
       "MCD     2856\n",
       "MMM     2856\n",
       "MRK     2856\n",
       "MSFT    2856\n",
       "NKE     2856\n",
       "PG      2856\n",
       "TRV     2856\n",
       "UNH     2856\n",
       "CRM     2856\n",
       "VZ      2856\n",
       "V       2856\n",
       "WBA     2856\n",
       "WMT     2856\n",
       "DIS     2856\n",
       "DOW      287\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tic.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqC6c40Zh1iH"
   },
   "source": [
    "# Part 4: Preprocess Data\n",
    "Data preprocessing is a crucial step for training a high quality machine learning model. We need to check for missing data and do feature engineering in order to convert the data into a model-ready state.\n",
    "* Add technical indicators. In practical trading, various information needs to be taken into account, for example the historical stock prices, current holding shares, technical indicators, etc. In this article, we demonstrate two trend-following technical indicators: MACD and RSI.\n",
    "* Add turbulence index. Risk-aversion reflects whether an investor will choose to preserve the capital. It also influences one's trading strategy when facing different market volatility level. To control the risk in a worst-case scenario, such as financial crisis of 2007–2008, FinRL employs the financial turbulence index that measures extreme asset price fluctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T13:26:53.750127Z",
     "iopub.status.busy": "2025-05-09T13:26:53.749852Z",
     "iopub.status.idle": "2025-05-09T13:26:53.752628Z",
     "shell.execute_reply": "2025-05-09T13:26:53.752067Z",
     "shell.execute_reply.started": "2025-05-09T13:26:53.750107Z"
    },
    "id": "kM5bH9uroCeg"
   },
   "outputs": [],
   "source": [
    "#  INDICATORS = ['macd',\n",
    "#                'rsi_30',\n",
    "#                'cci_30',\n",
    "#                'dx_30']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-09T13:26:53.753639Z",
     "iopub.status.busy": "2025-05-09T13:26:53.753388Z",
     "iopub.status.idle": "2025-05-09T13:27:22.083908Z",
     "shell.execute_reply": "2025-05-09T13:27:22.083265Z",
     "shell.execute_reply.started": "2025-05-09T13:26:53.753622Z"
    },
    "id": "jgXfBcjxtj1a",
    "outputId": "89760481-c993-4b3b-b804-879a96035f06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added technical indicators\n",
      "Successfully added turbulence index\n"
     ]
    }
   ],
   "source": [
    "fe = FeatureEngineer(use_technical_indicator=True,\n",
    "                     tech_indicator_list = INDICATORS,\n",
    "                     use_turbulence=True,\n",
    "                     user_defined_feature = False)\n",
    "\n",
    "processed = fe.preprocess_data(df)\n",
    "processed = processed.copy()\n",
    "processed = processed.fillna(0)\n",
    "processed = processed.replace(np.inf,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "execution": {
     "iopub.execute_input": "2025-05-09T13:27:22.084994Z",
     "iopub.status.busy": "2025-05-09T13:27:22.084699Z",
     "iopub.status.idle": "2025-05-09T13:27:22.100090Z",
     "shell.execute_reply": "2025-05-09T13:27:22.099269Z",
     "shell.execute_reply.started": "2025-05-09T13:27:22.084973Z"
    },
    "id": "grvhGJJII3Xn",
    "outputId": "f2e5016e-b268-4074-c01c-ddb95de82407"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>volume</th>\n",
       "      <th>tic</th>\n",
       "      <th>day</th>\n",
       "      <th>macd</th>\n",
       "      <th>boll_ub</th>\n",
       "      <th>boll_lb</th>\n",
       "      <th>rsi_30</th>\n",
       "      <th>cci_30</th>\n",
       "      <th>dx_30</th>\n",
       "      <th>close_30_sma</th>\n",
       "      <th>close_60_sma</th>\n",
       "      <th>turbulence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72976</th>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>82.740120</td>\n",
       "      <td>83.293109</td>\n",
       "      <td>80.212172</td>\n",
       "      <td>80.441983</td>\n",
       "      <td>4434935</td>\n",
       "      <td>IBM</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.838269</td>\n",
       "      <td>90.919810</td>\n",
       "      <td>77.257832</td>\n",
       "      <td>42.655690</td>\n",
       "      <td>-75.076507</td>\n",
       "      <td>19.297287</td>\n",
       "      <td>84.862049</td>\n",
       "      <td>88.258245</td>\n",
       "      <td>43.730012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78290</th>\n",
       "      <td>2019-09-24</td>\n",
       "      <td>67.399071</td>\n",
       "      <td>68.431019</td>\n",
       "      <td>67.229770</td>\n",
       "      <td>68.164974</td>\n",
       "      <td>9377085</td>\n",
       "      <td>MRK</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.096572</td>\n",
       "      <td>70.421542</td>\n",
       "      <td>65.220139</td>\n",
       "      <td>50.548718</td>\n",
       "      <td>-23.825686</td>\n",
       "      <td>5.324453</td>\n",
       "      <td>68.020201</td>\n",
       "      <td>67.351402</td>\n",
       "      <td>14.162461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15946</th>\n",
       "      <td>2011-03-09</td>\n",
       "      <td>16.662004</td>\n",
       "      <td>16.876751</td>\n",
       "      <td>16.569326</td>\n",
       "      <td>16.817978</td>\n",
       "      <td>17861600</td>\n",
       "      <td>V</td>\n",
       "      <td>3</td>\n",
       "      <td>0.082322</td>\n",
       "      <td>17.334211</td>\n",
       "      <td>16.340403</td>\n",
       "      <td>50.483953</td>\n",
       "      <td>15.838802</td>\n",
       "      <td>1.668754</td>\n",
       "      <td>16.614524</td>\n",
       "      <td>16.357676</td>\n",
       "      <td>34.018023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43374</th>\n",
       "      <td>2014-12-10</td>\n",
       "      <td>41.514187</td>\n",
       "      <td>41.887941</td>\n",
       "      <td>41.320391</td>\n",
       "      <td>41.611088</td>\n",
       "      <td>13889878</td>\n",
       "      <td>MRK</td>\n",
       "      <td>3</td>\n",
       "      <td>0.417339</td>\n",
       "      <td>42.633769</td>\n",
       "      <td>40.481116</td>\n",
       "      <td>53.095091</td>\n",
       "      <td>49.664220</td>\n",
       "      <td>6.145048</td>\n",
       "      <td>41.226717</td>\n",
       "      <td>40.674279</td>\n",
       "      <td>37.495327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30082</th>\n",
       "      <td>2013-02-15</td>\n",
       "      <td>123.315109</td>\n",
       "      <td>124.548330</td>\n",
       "      <td>122.591081</td>\n",
       "      <td>124.436942</td>\n",
       "      <td>4254400</td>\n",
       "      <td>GS</td>\n",
       "      <td>5</td>\n",
       "      <td>4.115029</td>\n",
       "      <td>124.813330</td>\n",
       "      <td>112.625768</td>\n",
       "      <td>71.706680</td>\n",
       "      <td>121.556211</td>\n",
       "      <td>76.602174</td>\n",
       "      <td>115.333332</td>\n",
       "      <td>106.202992</td>\n",
       "      <td>34.888960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             date       close        high         low        open    volume  \\\n",
       "72976  2019-01-02   82.740120   83.293109   80.212172   80.441983   4434935   \n",
       "78290  2019-09-24   67.399071   68.431019   67.229770   68.164974   9377085   \n",
       "15946  2011-03-09   16.662004   16.876751   16.569326   16.817978  17861600   \n",
       "43374  2014-12-10   41.514187   41.887941   41.320391   41.611088  13889878   \n",
       "30082  2013-02-15  123.315109  124.548330  122.591081  124.436942   4254400   \n",
       "\n",
       "       tic  day      macd     boll_ub     boll_lb     rsi_30      cci_30  \\\n",
       "72976  IBM    3 -1.838269   90.919810   77.257832  42.655690  -75.076507   \n",
       "78290  MRK    2 -0.096572   70.421542   65.220139  50.548718  -23.825686   \n",
       "15946    V    3  0.082322   17.334211   16.340403  50.483953   15.838802   \n",
       "43374  MRK    3  0.417339   42.633769   40.481116  53.095091   49.664220   \n",
       "30082   GS    5  4.115029  124.813330  112.625768  71.706680  121.556211   \n",
       "\n",
       "           dx_30  close_30_sma  close_60_sma  turbulence  \n",
       "72976  19.297287     84.862049     88.258245   43.730012  \n",
       "78290   5.324453     68.020201     67.351402   14.162461  \n",
       "15946   1.668754     16.614524     16.357676   34.018023  \n",
       "43374   6.145048     41.226717     40.674279   37.495327  \n",
       "30082  76.602174    115.333332    106.202992   34.888960  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-QsYaY0Dh1iw"
   },
   "source": [
    "<a id='4'></a>\n",
    "# Part 5. Design Environment\n",
    "Considering the stochastic and interactive nature of the automated stock trading tasks, a financial task is modeled as a **Markov Decision Process (MDP)** problem. The training process involves observing stock price change, taking an action and reward's calculation to have the agent adjusting its strategy accordingly. By interacting with the environment, the trading agent will derive a trading strategy with the maximized rewards as time proceeds.\n",
    "\n",
    "Our trading environments, based on OpenAI Gym framework, simulate live stock markets with real market data according to the principle of time-driven simulation.\n",
    "\n",
    "The action space describes the allowed actions that the agent interacts with the environment. Normally, action a includes three actions: {-1, 0, 1}, where -1, 0, 1 represent selling, holding, and buying one share. Also, an action can be carried upon multiple shares. We use an action space {-k,…,-1, 0, 1, …, k}, where k denotes the number of shares to buy and -k denotes the number of shares to sell. For example, \"Buy 10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or -10, respectively. The continuous action space needs to be normalized to [-1, 1], since the policy is defined on a Gaussian distribution, which needs to be normalized and symmetric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-09T13:27:22.101526Z",
     "iopub.status.busy": "2025-05-09T13:27:22.101136Z",
     "iopub.status.idle": "2025-05-09T13:27:22.110056Z",
     "shell.execute_reply": "2025-05-09T13:27:22.109308Z",
     "shell.execute_reply.started": "2025-05-09T13:27:22.101493Z"
    },
    "id": "Q2zqII8rMIqn",
    "outputId": "ef6b8f80-bb06-4447-d790-4ff9d4bfa7c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 29, State Space: 291\n"
     ]
    }
   ],
   "source": [
    "stock_dimension = len(processed.tic.unique())\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T13:27:22.111384Z",
     "iopub.status.busy": "2025-05-09T13:27:22.111001Z",
     "iopub.status.idle": "2025-05-09T13:27:22.115947Z",
     "shell.execute_reply": "2025-05-09T13:27:22.115153Z",
     "shell.execute_reply.started": "2025-05-09T13:27:22.111351Z"
    },
    "id": "AWyp84Ltto19"
   },
   "outputs": [],
   "source": [
    "env_kwargs = {\n",
    "    \"hmax\": 100,\n",
    "    \"initial_amount\": 1000000,\n",
    "    \"buy_cost_pct\": 0.001,\n",
    "    \"sell_cost_pct\": 0.001,\n",
    "    \"state_space\": state_space,\n",
    "    \"stock_dim\": stock_dimension,\n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": stock_dimension,\n",
    "    \"reward_scaling\": 1e-4,\n",
    "    \"print_verbosity\":5\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMNR5nHjh1iz"
   },
   "source": [
    "<a id='5'></a>\n",
    "# Part 6: Implement DRL Algorithms\n",
    "* The implementation of the DRL algorithms are based on **OpenAI Baselines** and **Stable Baselines**. Stable Baselines is a fork of OpenAI Baselines, with a major structural refactoring, and code cleanups.\n",
    "* FinRL library includes fine-tuned standard DRL algorithms, such as DQN, DDPG,\n",
    "Multi-Agent DDPG, PPO, SAC, A2C and TD3. We also allow users to\n",
    "design their own DRL algorithms by adapting these DRL algorithms.\n",
    "\n",
    "* In this notebook, we are training and validating 3 agents (A2C, PPO, DDPG) using Rolling-window Ensemble Method ([reference code](https://github.com/AI4Finance-LLC/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020/blob/80415db8fa7b2179df6bd7e81ce4fe8dbf913806/model/models.py#L92))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T13:27:22.117296Z",
     "iopub.status.busy": "2025-05-09T13:27:22.116893Z",
     "iopub.status.idle": "2025-05-09T13:27:22.142779Z",
     "shell.execute_reply": "2025-05-09T13:27:22.142055Z",
     "shell.execute_reply.started": "2025-05-09T13:27:22.117263Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据日期范围: 2009-01-02 到 2020-05-07\n",
      "测试日期范围: 2015-10-01 到 2020-05-08\n",
      "数据中的日期列格式: object\n",
      "测试日期格式: <class 'str'> <class 'str'>\n",
      "测试期间的数据量: 33840\n"
     ]
    }
   ],
   "source": [
    "# 在创建ensemble_agent之前添加\n",
    "print(\"数据日期范围:\", df.date.min(), \"到\", df.date.max())\n",
    "print(\"测试日期范围:\", TEST_START_DATE, \"到\", TEST_END_DATE)\n",
    "print(\"数据中的日期列格式:\", df.date.dtype)\n",
    "print(\"测试日期格式:\", type(TEST_START_DATE), type(TEST_END_DATE))\n",
    "\n",
    "# 检查数据是否为空\n",
    "print(\"测试期间的数据量:\", len(df[(df.date > TEST_START_DATE) & (df.date <= TEST_END_DATE)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T13:27:22.143979Z",
     "iopub.status.busy": "2025-05-09T13:27:22.143595Z",
     "iopub.status.idle": "2025-05-09T13:27:22.160746Z",
     "shell.execute_reply": "2025-05-09T13:27:22.160195Z",
     "shell.execute_reply.started": "2025-05-09T13:27:22.143931Z"
    },
    "id": "v-gthCxMtj1d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "rebalance_window = 63 # rebalance_window is the number of days to retrain the model\n",
    "validation_window = 63 # validation_window is the number of days to do validation and trading (e.g. if validation_window=63, then both validation and trading period will be 63 days)\n",
    "\n",
    "ensemble_agent = DRLEnsembleAgent(df=processed,\n",
    "                 train_period=(TRAIN_START_DATE,TRAIN_END_DATE),\n",
    "                 val_test_period=(TEST_START_DATE,TEST_END_DATE),\n",
    "                 rebalance_window=rebalance_window,\n",
    "                 validation_window=validation_window,\n",
    "                 **env_kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T13:27:22.161909Z",
     "iopub.status.busy": "2025-05-09T13:27:22.161545Z",
     "iopub.status.idle": "2025-05-09T13:27:22.167757Z",
     "shell.execute_reply": "2025-05-09T13:27:22.167037Z",
     "shell.execute_reply.started": "2025-05-09T13:27:22.161876Z"
    },
    "id": "KsfEHa_Etj1d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "A2C_model_kwargs = {\n",
    "                    'n_steps': 5,\n",
    "                    'ent_coef': 0.005,\n",
    "                    'learning_rate': 0.0007\n",
    "                    }\n",
    "\n",
    "PPO_model_kwargs = {\n",
    "                    \"ent_coef\":0.01,\n",
    "                    \"n_steps\": 2048,\n",
    "                    \"learning_rate\": 0.00025,\n",
    "                    \"batch_size\": 128\n",
    "                    }\n",
    "\n",
    "DDPG_model_kwargs = {\n",
    "                      #\"action_noise\":\"ornstein_uhlenbeck\",\n",
    "                      \"buffer_size\": 10_000,\n",
    "                      \"learning_rate\": 0.0005,\n",
    "                      \"batch_size\": 64\n",
    "                    }\n",
    "\n",
    "SAC_model_kwargs = {\n",
    "    \"batch_size\": 64,\n",
    "    \"buffer_size\": 100000,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"learning_starts\": 100,\n",
    "    \"ent_coef\": \"auto_0.1\",\n",
    "}\n",
    "\n",
    "TD3_model_kwargs = {\"batch_size\": 100, \"buffer_size\": 1000000, \"learning_rate\": 0.0001}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "timesteps_dict = {'a2c' : 10_000,\n",
    "                 'ppo' : 10_000,\n",
    "                 'ddpg' : 10_000,\n",
    "                 'sac' : 10_000,\n",
    "                 'td3' : 10_000\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-09T13:27:22.169127Z",
     "iopub.status.busy": "2025-05-09T13:27:22.168747Z",
     "iopub.status.idle": "2025-05-09T16:11:06.012129Z",
     "shell.execute_reply": "2025-05-09T16:11:06.011210Z",
     "shell.execute_reply.started": "2025-05-09T13:27:22.169095Z"
    },
    "id": "_1lyCECstj1e",
    "outputId": "73986412-160f-4ee4-adf1-9ee740e5f1c4",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============Start Ensemble Strategy============\n",
      "============================================\n",
      "turbulence_threshold:  160.3155295025199\n",
      "======Model training from:  2009-01-01 to  2015-10-02\n",
      "======a2c Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_126_3\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 123        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | 0.194      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -40.5      |\n",
      "|    reward             | 0.39653528 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 1.31       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 132        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | -0.662     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -45.6      |\n",
      "|    reward             | -1.1517683 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 1.76       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 136       |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | -0.0112   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -213      |\n",
      "|    reward             | 4.7987885 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 29.2      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 138        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 14         |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0.0205     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | 28.2       |\n",
      "|    reward             | 0.68774754 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.18       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 139       |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 17        |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | 0.13      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | 36.5      |\n",
      "|    reward             | 2.2792141 |\n",
      "|    std                | 0.999     |\n",
      "|    value_loss         | 1.21      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 140       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 21        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | 86.5      |\n",
      "|    reward             | 0.1545174 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 7.56      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 140       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 24        |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | -0.0795   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -82.7     |\n",
      "|    reward             | 2.5604987 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 5.11      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 141       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 28        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -58.7     |\n",
      "|    reward             | -1.831296 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 3.07      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 141       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 31        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 117       |\n",
      "|    reward             | 1.8256629 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 7.38      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 141        |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 35         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | 250        |\n",
      "|    reward             | 0.27617827 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 37         |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 141         |\n",
      "|    iterations         | 1100        |\n",
      "|    time_elapsed       | 38          |\n",
      "|    total_timesteps    | 5500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.3       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1099        |\n",
      "|    policy_loss        | 26.3        |\n",
      "|    reward             | -0.34792334 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 1.58        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 142         |\n",
      "|    iterations         | 1200        |\n",
      "|    time_elapsed       | 42          |\n",
      "|    total_timesteps    | 6000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.3       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1199        |\n",
      "|    policy_loss        | 144         |\n",
      "|    reward             | -0.68789715 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 15          |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 142        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 45         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | -150       |\n",
      "|    reward             | -6.5882864 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 15.4       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 142        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 49         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | 47         |\n",
      "|    reward             | -2.9174845 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.52       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 142       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 52        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 226       |\n",
      "|    reward             | 4.3334823 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 37.5      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 142       |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 55        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 2.98e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -245      |\n",
      "|    reward             | -1.405451 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 45.5      |\n",
      "-------------------------------------\n",
      "day: 1698, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2955336.93\n",
      "total_reward: 1955336.93\n",
      "total_cost: 28790.55\n",
      "total_trades: 33406\n",
      "Sharpe: 0.936\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 142         |\n",
      "|    iterations         | 1700        |\n",
      "|    time_elapsed       | 59          |\n",
      "|    total_timesteps    | 8500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.3       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1699        |\n",
      "|    policy_loss        | -376        |\n",
      "|    reward             | -0.19307749 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 108         |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 143      |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 62       |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.3    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | -29.4    |\n",
      "|    reward             | 1.280865 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 0.68     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 143       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 66        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -236      |\n",
      "|    reward             | 3.7379866 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 31.4      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 143       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 69        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -69.7     |\n",
      "|    reward             | 3.5854225 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 23.4      |\n",
      "-------------------------------------\n",
      "======a2c Validation from:  2015-10-02 to  2016-01-04\n",
      "a2c Sharpe Ratio:  -0.04286544877936798\n",
      "======ddpg Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_126_3\n",
      "day: 1698, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3482467.11\n",
      "total_reward: 2482467.11\n",
      "total_cost: 999.00\n",
      "total_trades: 28835\n",
      "Sharpe: 1.062\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 90        |\n",
      "|    time_elapsed    | 75        |\n",
      "|    total_timesteps | 6796      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -313      |\n",
      "|    critic_loss     | 565       |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 6695      |\n",
      "|    reward          | 3.1634452 |\n",
      "----------------------------------\n",
      "======ddpg Validation from:  2015-10-02 to  2016-01-04\n",
      "ddpg Sharpe Ratio:  -0.015455635159906273\n",
      "======td3 Training========\n",
      "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.0001}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/td3/td3_126_3\n",
      "day: 1698, episode: 15\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3217571.49\n",
      "total_reward: 2217571.49\n",
      "total_cost: 1618.81\n",
      "total_trades: 28472\n",
      "Sharpe: 1.115\n",
      "=================================\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 93         |\n",
      "|    time_elapsed    | 72         |\n",
      "|    total_timesteps | 6796       |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 401        |\n",
      "|    critic_loss     | 467        |\n",
      "|    learning_rate   | 0.0001     |\n",
      "|    n_updates       | 6695       |\n",
      "|    reward          | 0.34948996 |\n",
      "-----------------------------------\n",
      "======td3 Validation from:  2015-10-02 to  2016-01-04\n",
      "td3 Sharpe Ratio:  0.004611039957432065\n",
      "======sac Training========\n",
      "{'batch_size': 64, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/sac/sac_126_3\n",
      "day: 1698, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3333802.30\n",
      "total_reward: 2333802.30\n",
      "total_cost: 2884.77\n",
      "total_trades: 23114\n",
      "Sharpe: 1.116\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 63        |\n",
      "|    time_elapsed    | 106       |\n",
      "|    total_timesteps | 6796      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.8e+03   |\n",
      "|    critic_loss     | 370       |\n",
      "|    ent_coef        | 0.182     |\n",
      "|    ent_coef_loss   | 310       |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 6695      |\n",
      "|    reward          | 4.2139835 |\n",
      "----------------------------------\n",
      "======sac Validation from:  2015-10-02 to  2016-01-04\n",
      "sac Sharpe Ratio:  -0.03920181676903267\n",
      "======ppo Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_126_3\n",
      "day: 1698, episode: 25\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2267122.76\n",
      "total_reward: 1267122.76\n",
      "total_cost: 130729.51\n",
      "total_trades: 47978\n",
      "Sharpe: 0.953\n",
      "=================================\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 162         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 12          |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | -0.98004764 |\n",
      "------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 157         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017275598 |\n",
      "|    clip_fraction        | 0.219       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | 0.00674     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 4.93        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0223     |\n",
      "|    reward               | 3.7323027   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 9.92        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 155         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 39          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017899951 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | -0.0252     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 5.1         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0251     |\n",
      "|    reward               | 0.6744521   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 12.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 155         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 52          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016135175 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | 0.00952     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 7.38        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.021      |\n",
      "|    reward               | 1.1377735   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 15.1        |\n",
      "-----------------------------------------\n",
      "day: 1698, episode: 30\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2414188.95\n",
      "total_reward: 1414188.95\n",
      "total_cost: 130358.21\n",
      "total_trades: 47653\n",
      "Sharpe: 0.943\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 155         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 65          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016375616 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | 0.0368      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 3.05        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0169     |\n",
      "|    reward               | 0.291832    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 12.1        |\n",
      "-----------------------------------------\n",
      "======ppo Validation from:  2015-10-02 to  2016-01-04\n",
      "ppo Sharpe Ratio:  -0.06822161754874773\n",
      "======Best Model Retraining from:  2009-01-01 to  2016-01-04\n",
      "======Trading from:  2016-01-04 to  2016-04-05\n",
      "============================================\n",
      "turbulence_threshold:  160.3155295025199\n",
      "======Model training from:  2009-01-01 to  2016-01-04\n",
      "======a2c Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_189_3\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 140        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | -0.101     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -39.1      |\n",
      "|    reward             | 0.59505755 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 0.899      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 140        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -42.6      |\n",
      "|    reward             | -1.4470199 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.63       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 140       |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0.0899    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -308      |\n",
      "|    reward             | 6.5055876 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 65        |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 140        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 14         |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | -0.00382   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -60        |\n",
      "|    reward             | 0.73619246 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.43       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 140       |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 17        |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0.0516    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | 115       |\n",
      "|    reward             | 1.0577819 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 10.7      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 140        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 21         |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | 149        |\n",
      "|    reward             | -0.5646447 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 15.4       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 140        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 24         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | 210        |\n",
      "|    reward             | 0.61991817 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 32.3       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 140        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 28         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | -170       |\n",
      "|    reward             | 0.15058401 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 21.7       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 140        |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 32         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | -158       |\n",
      "|    reward             | 0.27921203 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 21.5       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 140       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 35        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 186       |\n",
      "|    reward             | 0.5662085 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 27.9      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 140        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 39         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | -105       |\n",
      "|    reward             | 0.50887686 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 7.31       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 140        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 42         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | -9.28      |\n",
      "|    reward             | -2.7948372 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.51       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 140        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 46         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 72.5       |\n",
      "|    reward             | 0.67666453 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 5.3        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 140        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 49         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | -232       |\n",
      "|    reward             | 0.29722792 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 61.4       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 140        |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 53         |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | -4.1       |\n",
      "|    reward             | -1.5456113 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 0.049      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 140        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 56         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | 47.1       |\n",
      "|    reward             | -0.4772999 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.85       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 140        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 60         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | -58.5      |\n",
      "|    reward             | -3.4792593 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 2.54       |\n",
      "--------------------------------------\n",
      "day: 1761, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2637656.74\n",
      "total_reward: 1637656.74\n",
      "total_cost: 26030.45\n",
      "total_trades: 30225\n",
      "Sharpe: 0.949\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 140       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 63        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | 0.0528    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -6.37     |\n",
      "|    reward             | 0.6474523 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.36      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 140        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 67         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | -165       |\n",
      "|    reward             | -1.1150519 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 16.2       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 140         |\n",
      "|    iterations         | 2000        |\n",
      "|    time_elapsed       | 71          |\n",
      "|    total_timesteps    | 10000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.4       |\n",
      "|    explained_variance | 0.00272     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1999        |\n",
      "|    policy_loss        | 117         |\n",
      "|    reward             | -0.25340548 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 8.28        |\n",
      "---------------------------------------\n",
      "======a2c Validation from:  2016-01-04 to  2016-04-05\n",
      "a2c Sharpe Ratio:  0.31054248190781425\n",
      "======ddpg Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_189_3\n",
      "day: 1761, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3051534.84\n",
      "total_reward: 2051534.84\n",
      "total_cost: 1215.76\n",
      "total_trades: 24782\n",
      "Sharpe: 0.931\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 88        |\n",
      "|    time_elapsed    | 79        |\n",
      "|    total_timesteps | 7048      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -109      |\n",
      "|    critic_loss     | 351       |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 6947      |\n",
      "|    reward          | -3.180884 |\n",
      "----------------------------------\n",
      "======ddpg Validation from:  2016-01-04 to  2016-04-05\n",
      "ddpg Sharpe Ratio:  0.16312515547570944\n",
      "======td3 Training========\n",
      "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.0001}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/td3/td3_189_3\n",
      "day: 1761, episode: 15\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2743769.85\n",
      "total_reward: 1743769.85\n",
      "total_cost: 1058.90\n",
      "total_trades: 22926\n",
      "Sharpe: 0.913\n",
      "=================================\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 91         |\n",
      "|    time_elapsed    | 76         |\n",
      "|    total_timesteps | 7048       |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 76         |\n",
      "|    critic_loss     | 92.3       |\n",
      "|    learning_rate   | 0.0001     |\n",
      "|    n_updates       | 6947       |\n",
      "|    reward          | -2.6680768 |\n",
      "-----------------------------------\n",
      "======td3 Validation from:  2016-01-04 to  2016-04-05\n",
      "td3 Sharpe Ratio:  0.19780091196489347\n",
      "======sac Training========\n",
      "{'batch_size': 64, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/sac/sac_189_3\n",
      "day: 1761, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2600813.84\n",
      "total_reward: 1600813.84\n",
      "total_cost: 2059.13\n",
      "total_trades: 23670\n",
      "Sharpe: 0.943\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 63        |\n",
      "|    time_elapsed    | 111       |\n",
      "|    total_timesteps | 7048      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.13e+03  |\n",
      "|    critic_loss     | 102       |\n",
      "|    ent_coef        | 0.185     |\n",
      "|    ent_coef_loss   | 255       |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 6947      |\n",
      "|    reward          | -4.874497 |\n",
      "----------------------------------\n",
      "======sac Validation from:  2016-01-04 to  2016-04-05\n",
      "sac Sharpe Ratio:  0.19799713857631596\n",
      "======ppo Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_189_3\n",
      "day: 1761, episode: 25\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2149631.48\n",
      "total_reward: 1149631.48\n",
      "total_cost: 138757.80\n",
      "total_trades: 49663\n",
      "Sharpe: 0.847\n",
      "=================================\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 158         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 12          |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | -0.90507597 |\n",
      "------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 154         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017129058 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | -0.00717    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 3.86        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0223     |\n",
      "|    reward               | 0.003237404 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 8.92        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 152        |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 40         |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01681919 |\n",
      "|    clip_fraction        | 0.159      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -41.2      |\n",
      "|    explained_variance   | -0.0304    |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 4.19       |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0202    |\n",
      "|    reward               | -1.6210777 |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 12.4       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 152         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 53          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011872945 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | 0.00351     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 4.21        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0222     |\n",
      "|    reward               | -1.3392339  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 10          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 151         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 67          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015236686 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | 0.00218     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 4.03        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0241     |\n",
      "|    reward               | -2.0310578  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 12.6        |\n",
      "-----------------------------------------\n",
      "======ppo Validation from:  2016-01-04 to  2016-04-05\n",
      "ppo Sharpe Ratio:  0.16745153315544423\n",
      "======Best Model Retraining from:  2009-01-01 to  2016-04-05\n",
      "======Trading from:  2016-04-05 to  2016-07-05\n",
      "============================================\n",
      "turbulence_threshold:  160.3155295025199\n",
      "======Model training from:  2009-01-01 to  2016-04-05\n",
      "======a2c Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_252_3\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 136         |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 3           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.1       |\n",
      "|    explained_variance | 0.0758      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | 23.6        |\n",
      "|    reward             | -0.40107942 |\n",
      "|    std                | 0.999       |\n",
      "|    value_loss         | 0.509       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 136        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | -0.0395    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -23.7      |\n",
      "|    reward             | -2.2200377 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 1.41       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 136       |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0.0525    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -221      |\n",
      "|    reward             | 4.6903124 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 30.2      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 136        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 14         |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | -0.164     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -5.62      |\n",
      "|    reward             | 0.89666754 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.08       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 136       |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 18        |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0.0131    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | 219       |\n",
      "|    reward             | 3.7849367 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 29.5      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 136       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 21        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | -0.342    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -147      |\n",
      "|    reward             | 1.0086087 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 13.3      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 136        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 25         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0.0196     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -586       |\n",
      "|    reward             | -0.4041996 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 212        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 136         |\n",
      "|    iterations         | 800         |\n",
      "|    time_elapsed       | 29          |\n",
      "|    total_timesteps    | 4000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.3       |\n",
      "|    explained_variance | -0.0212     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 799         |\n",
      "|    policy_loss        | -151        |\n",
      "|    reward             | -0.07363733 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 16.4        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 136        |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 32         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | -55.1      |\n",
      "|    reward             | -1.8250048 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 2.81       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 137        |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 36         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | -0.647     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | 33.7       |\n",
      "|    reward             | -1.9279289 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.83       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 136        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 40         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | -22.5      |\n",
      "|    reward             | 0.14023297 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 0.502      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 136        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 43         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0.0716     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | 31.7       |\n",
      "|    reward             | 0.49218065 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.16       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 136       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 47        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0.0901    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | 98.2      |\n",
      "|    reward             | -1.178661 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 7         |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 136         |\n",
      "|    iterations         | 1400        |\n",
      "|    time_elapsed       | 51          |\n",
      "|    total_timesteps    | 7000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.3       |\n",
      "|    explained_variance | -0.183      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1399        |\n",
      "|    policy_loss        | 79.1        |\n",
      "|    reward             | -0.10023377 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 8.9         |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 136       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 54        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | -0.0287   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 71.7      |\n",
      "|    reward             | 1.4595218 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 4.4       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 136       |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 58        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | 0.758     |\n",
      "|    reward             | 2.4257278 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 6.1       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 136        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 62         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | -41        |\n",
      "|    reward             | 0.36311683 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.96       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 136       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 65        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0.000621  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -36.4     |\n",
      "|    reward             | 3.7333708 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 12.4      |\n",
      "-------------------------------------\n",
      "day: 1824, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3940672.77\n",
      "total_reward: 2940672.77\n",
      "total_cost: 80262.10\n",
      "total_trades: 41535\n",
      "Sharpe: 1.216\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 136        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 69         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 0.101      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | -35        |\n",
      "|    reward             | -1.2541943 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.14       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 136       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 73        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | -0.187    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 125       |\n",
      "|    reward             | 1.2249546 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 11        |\n",
      "-------------------------------------\n",
      "======a2c Validation from:  2016-04-05 to  2016-07-05\n",
      "a2c Sharpe Ratio:  0.04764384591858919\n",
      "======ddpg Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_252_3\n",
      "day: 1824, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3132133.69\n",
      "total_reward: 2132133.69\n",
      "total_cost: 1178.12\n",
      "total_trades: 34765\n",
      "Sharpe: 0.970\n",
      "=================================\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 86         |\n",
      "|    time_elapsed    | 84         |\n",
      "|    total_timesteps | 7300       |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 149        |\n",
      "|    critic_loss     | 500        |\n",
      "|    learning_rate   | 0.0005     |\n",
      "|    n_updates       | 7199       |\n",
      "|    reward          | -0.8430929 |\n",
      "-----------------------------------\n",
      "======ddpg Validation from:  2016-04-05 to  2016-07-05\n",
      "ddpg Sharpe Ratio:  0.09619794243178269\n",
      "======td3 Training========\n",
      "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.0001}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/td3/td3_252_3\n",
      "day: 1824, episode: 15\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3884190.40\n",
      "total_reward: 2884190.40\n",
      "total_cost: 1298.67\n",
      "total_trades: 23713\n",
      "Sharpe: 1.124\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 90        |\n",
      "|    time_elapsed    | 80        |\n",
      "|    total_timesteps | 7300      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -123      |\n",
      "|    critic_loss     | 3.38e+03  |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 7199      |\n",
      "|    reward          | -2.257415 |\n",
      "----------------------------------\n",
      "======td3 Validation from:  2016-04-05 to  2016-07-05\n",
      "td3 Sharpe Ratio:  0.03220845269177387\n",
      "======sac Training========\n",
      "{'batch_size': 64, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/sac/sac_252_3\n",
      "day: 1824, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3686225.21\n",
      "total_reward: 2686225.21\n",
      "total_cost: 3768.06\n",
      "total_trades: 20247\n",
      "Sharpe: 1.101\n",
      "=================================\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 62         |\n",
      "|    time_elapsed    | 117        |\n",
      "|    total_timesteps | 7300       |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 1.46e+03   |\n",
      "|    critic_loss     | 123        |\n",
      "|    ent_coef        | 0.188      |\n",
      "|    ent_coef_loss   | 174        |\n",
      "|    learning_rate   | 0.0001     |\n",
      "|    n_updates       | 7199       |\n",
      "|    reward          | 0.87159956 |\n",
      "-----------------------------------\n",
      "======sac Validation from:  2016-04-05 to  2016-07-05\n",
      "sac Sharpe Ratio:  -0.04644904637707228\n",
      "======ppo Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_252_3\n",
      "day: 1824, episode: 25\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2468286.25\n",
      "total_reward: 1468286.25\n",
      "total_cost: 145686.72\n",
      "total_trades: 51463\n",
      "Sharpe: 0.920\n",
      "=================================\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 153         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 13          |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | -0.75191945 |\n",
      "------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 150         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017471941 |\n",
      "|    clip_fraction        | 0.223       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | -0.0441     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 4.23        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0228     |\n",
      "|    reward               | -0.24388963 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 12.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 149         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 41          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016495083 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | -0.00731    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 4.02        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0138     |\n",
      "|    reward               | 3.293367    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 10          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 148         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 55          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014078644 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | 0.0201      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 5.94        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0205     |\n",
      "|    reward               | 1.2630249   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 13.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 148         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 68          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016636148 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | 0.00835     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 5.64        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0231     |\n",
      "|    reward               | -0.11090567 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 11.7        |\n",
      "-----------------------------------------\n",
      "======ppo Validation from:  2016-04-05 to  2016-07-05\n",
      "ppo Sharpe Ratio:  0.023557579121099268\n",
      "======Best Model Retraining from:  2009-01-01 to  2016-07-05\n",
      "======Trading from:  2016-07-05 to  2016-10-03\n",
      "============================================\n",
      "turbulence_threshold:  160.3155295025199\n",
      "======Model training from:  2009-01-01 to  2016-07-05\n",
      "======a2c Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_315_3\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 133         |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 3           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41         |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -23.8       |\n",
      "|    reward             | -0.08614993 |\n",
      "|    std                | 0.996       |\n",
      "|    value_loss         | 0.96        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 133        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41        |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -8.64      |\n",
      "|    reward             | -2.1322906 |\n",
      "|    std                | 0.996      |\n",
      "|    value_loss         | 0.82       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 133       |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 11        |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41       |\n",
      "|    explained_variance | 0.023     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -197      |\n",
      "|    reward             | 4.0925117 |\n",
      "|    std                | 0.996     |\n",
      "|    value_loss         | 27        |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 133        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 14         |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -40.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | 40         |\n",
      "|    reward             | -2.0281005 |\n",
      "|    std                | 0.993      |\n",
      "|    value_loss         | 2.21       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 133        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 18         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -40.9      |\n",
      "|    explained_variance | -0.0661    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -85.2      |\n",
      "|    reward             | -1.0284133 |\n",
      "|    std                | 0.992      |\n",
      "|    value_loss         | 5.28       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 133       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 22        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -40.9     |\n",
      "|    explained_variance | -1.19e-06 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -24.1     |\n",
      "|    reward             | -2.574273 |\n",
      "|    std                | 0.991     |\n",
      "|    value_loss         | 2.34      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 134         |\n",
      "|    iterations         | 700         |\n",
      "|    time_elapsed       | 26          |\n",
      "|    total_timesteps    | 3500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -40.9       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 699         |\n",
      "|    policy_loss        | -7.85       |\n",
      "|    reward             | 0.084280506 |\n",
      "|    std                | 0.992       |\n",
      "|    value_loss         | 0.223       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 134        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 29         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41        |\n",
      "|    explained_variance | -0.174     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | 55.9       |\n",
      "|    reward             | -0.6595656 |\n",
      "|    std                | 0.994      |\n",
      "|    value_loss         | 5.42       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 134         |\n",
      "|    iterations         | 900         |\n",
      "|    time_elapsed       | 33          |\n",
      "|    total_timesteps    | 4500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41         |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 899         |\n",
      "|    policy_loss        | 88          |\n",
      "|    reward             | -0.14165804 |\n",
      "|    std                | 0.994       |\n",
      "|    value_loss         | 5.52        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 134       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 37        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 31        |\n",
      "|    reward             | 1.3683215 |\n",
      "|    std                | 0.994     |\n",
      "|    value_loss         | 1.49      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 134        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 41         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41        |\n",
      "|    explained_variance | 0.0233     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | -136       |\n",
      "|    reward             | -2.5382926 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 19.4       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 134        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 44         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41        |\n",
      "|    explained_variance | -0.0362    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | -65.4      |\n",
      "|    reward             | -0.3310422 |\n",
      "|    std                | 0.996      |\n",
      "|    value_loss         | 3.85       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 134        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 48         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41        |\n",
      "|    explained_variance | -0.264     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 151        |\n",
      "|    reward             | 0.21967782 |\n",
      "|    std                | 0.994      |\n",
      "|    value_loss         | 21         |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 134       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 52        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -40.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 148       |\n",
      "|    reward             | -1.591031 |\n",
      "|    std                | 0.992     |\n",
      "|    value_loss         | 15.4      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 134        |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 55         |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -40.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | 88.7       |\n",
      "|    reward             | -1.8315215 |\n",
      "|    std                | 0.992      |\n",
      "|    value_loss         | 8.89       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 134       |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 59        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -40.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | 39.3      |\n",
      "|    reward             | 0.9872737 |\n",
      "|    std                | 0.992     |\n",
      "|    value_loss         | 1.43      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 134      |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 63       |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41      |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | -35.3    |\n",
      "|    reward             | 0.750455 |\n",
      "|    std                | 0.994    |\n",
      "|    value_loss         | 1.1      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 134       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 67        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -91.7     |\n",
      "|    reward             | 3.7449634 |\n",
      "|    std                | 0.994     |\n",
      "|    value_loss         | 7.56      |\n",
      "-------------------------------------\n",
      "day: 1887, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2578349.72\n",
      "total_reward: 1578349.72\n",
      "total_cost: 12730.19\n",
      "total_trades: 35043\n",
      "Sharpe: 0.943\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 134        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 70         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | 44.5       |\n",
      "|    reward             | 0.89014393 |\n",
      "|    std                | 0.994      |\n",
      "|    value_loss         | 2.61       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 134       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 74        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41       |\n",
      "|    explained_variance | -0.0297   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -11.8     |\n",
      "|    reward             | 0.4774695 |\n",
      "|    std                | 0.995     |\n",
      "|    value_loss         | 3.12      |\n",
      "-------------------------------------\n",
      "======a2c Validation from:  2016-07-05 to  2016-10-03\n",
      "a2c Sharpe Ratio:  0.11702935417271812\n",
      "======ddpg Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_315_3\n",
      "day: 1887, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2866347.31\n",
      "total_reward: 1866347.31\n",
      "total_cost: 1160.98\n",
      "total_trades: 11436\n",
      "Sharpe: 0.774\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 85        |\n",
      "|    time_elapsed    | 87        |\n",
      "|    total_timesteps | 7552      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 11        |\n",
      "|    critic_loss     | 16.5      |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 7451      |\n",
      "|    reward          | 0.3035559 |\n",
      "----------------------------------\n",
      "======ddpg Validation from:  2016-07-05 to  2016-10-03\n",
      "ddpg Sharpe Ratio:  0.036067061094452645\n",
      "======td3 Training========\n",
      "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.0001}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/td3/td3_315_3\n",
      "day: 1887, episode: 15\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3260196.67\n",
      "total_reward: 2260196.67\n",
      "total_cost: 1277.72\n",
      "total_trades: 20863\n",
      "Sharpe: 0.973\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 87        |\n",
      "|    time_elapsed    | 85        |\n",
      "|    total_timesteps | 7552      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 332       |\n",
      "|    critic_loss     | 2.67e+03  |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 7451      |\n",
      "|    reward          | 0.3667506 |\n",
      "----------------------------------\n",
      "======td3 Validation from:  2016-07-05 to  2016-10-03\n",
      "td3 Sharpe Ratio:  -0.034952730903332334\n",
      "======sac Training========\n",
      "{'batch_size': 64, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/sac/sac_315_3\n",
      "day: 1887, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4298439.36\n",
      "total_reward: 3298439.36\n",
      "total_cost: 5597.28\n",
      "total_trades: 27864\n",
      "Sharpe: 1.034\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 61        |\n",
      "|    time_elapsed    | 122       |\n",
      "|    total_timesteps | 7552      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.16e+03  |\n",
      "|    critic_loss     | 87.5      |\n",
      "|    ent_coef        | 0.191     |\n",
      "|    ent_coef_loss   | 190       |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 7451      |\n",
      "|    reward          | 0.5054141 |\n",
      "----------------------------------\n",
      "======sac Validation from:  2016-07-05 to  2016-10-03\n",
      "sac Sharpe Ratio:  0.09058410562854183\n",
      "======ppo Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_315_3\n",
      "day: 1887, episode: 25\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1950210.51\n",
      "total_reward: 950210.51\n",
      "total_cost: 152933.78\n",
      "total_trades: 53081\n",
      "Sharpe: 0.684\n",
      "=================================\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 150        |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 13         |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.83390665 |\n",
      "-----------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 147         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014247601 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | -0.0275     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 3.1         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0234     |\n",
      "|    reward               | 0.58015794  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 8.68        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 145         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 42          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017391477 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | -0.00532    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 3.71        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0218     |\n",
      "|    reward               | -0.31426203 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 10.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 145         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 56          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018037118 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | 0.000567    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 6.53        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0181     |\n",
      "|    reward               | 1.8695658   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 13.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 144         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 70          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020348579 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | 0.0235      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 7.06        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0234     |\n",
      "|    reward               | 1.5828812   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 16.8        |\n",
      "-----------------------------------------\n",
      "======ppo Validation from:  2016-07-05 to  2016-10-03\n",
      "ppo Sharpe Ratio:  -0.08553977634640968\n",
      "======Best Model Retraining from:  2009-01-01 to  2016-10-03\n",
      "======Trading from:  2016-10-03 to  2017-01-03\n",
      "============================================\n",
      "turbulence_threshold:  160.3155295025199\n",
      "======Model training from:  2009-01-01 to  2016-10-03\n",
      "======a2c Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_378_3\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 130         |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 3           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.2       |\n",
      "|    explained_variance | -0.0356     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -14.4       |\n",
      "|    reward             | 0.009436992 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 1.53        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 130        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -23.3      |\n",
      "|    reward             | -1.7905687 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.04       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 131      |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 11       |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.2    |\n",
      "|    explained_variance | 1.81e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -161     |\n",
      "|    reward             | 7.781139 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 13.2     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 131         |\n",
      "|    iterations         | 400         |\n",
      "|    time_elapsed       | 15          |\n",
      "|    total_timesteps    | 2000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.1       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 399         |\n",
      "|    policy_loss        | -76.8       |\n",
      "|    reward             | -0.53936476 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 4.64        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 131       |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | -0.0348   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | 15.6      |\n",
      "|    reward             | 1.6850843 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 0.511     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 131        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 22         |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | 15.3       |\n",
      "|    reward             | -0.6118877 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.73       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 131      |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 26       |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.2    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 420      |\n",
      "|    reward             | 1.875967 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 111      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 131       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 30        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | 109       |\n",
      "|    reward             | 3.2569497 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 8.4       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 131       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 34        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 61        |\n",
      "|    reward             | 0.6740776 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 3.51      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 131       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 37        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 191       |\n",
      "|    reward             | 1.4321042 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 21.3      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 131       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 41        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -46.6     |\n",
      "|    reward             | -3.130736 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 34.2      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 131        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 45         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 0.205      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | 40.8       |\n",
      "|    reward             | 0.15530244 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 2.61       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 131         |\n",
      "|    iterations         | 1300        |\n",
      "|    time_elapsed       | 49          |\n",
      "|    total_timesteps    | 6500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.4       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1299        |\n",
      "|    policy_loss        | -103        |\n",
      "|    reward             | -0.45147428 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 6.1         |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 131       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 53        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 130       |\n",
      "|    reward             | 0.4838231 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 11        |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 131       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 56        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 537       |\n",
      "|    reward             | 1.7643106 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 221       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 131        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 60         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | -0.096     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | 34.7       |\n",
      "|    reward             | -1.4575342 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.4        |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 131      |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 64       |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.5    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | -85.8    |\n",
      "|    reward             | 1.973663 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 7.95     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 131         |\n",
      "|    iterations         | 1800        |\n",
      "|    time_elapsed       | 68          |\n",
      "|    total_timesteps    | 9000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.5       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1799        |\n",
      "|    policy_loss        | -19.5       |\n",
      "|    reward             | -0.98895323 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 1.06        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 131       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 72        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 0.00342   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -104      |\n",
      "|    reward             | 2.9267256 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 11.5      |\n",
      "-------------------------------------\n",
      "day: 1950, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4351690.80\n",
      "total_reward: 3351690.80\n",
      "total_cost: 10268.55\n",
      "total_trades: 31507\n",
      "Sharpe: 1.223\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 131       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 75        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 26.5      |\n",
      "|    reward             | 0.7625143 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 2.56      |\n",
      "-------------------------------------\n",
      "======a2c Validation from:  2016-10-03 to  2017-01-03\n",
      "a2c Sharpe Ratio:  0.4166170570219754\n",
      "======ddpg Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_378_3\n",
      "day: 1950, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3156447.12\n",
      "total_reward: 2156447.12\n",
      "total_cost: 999.00\n",
      "total_trades: 23400\n",
      "Sharpe: 0.901\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 84        |\n",
      "|    time_elapsed    | 92        |\n",
      "|    total_timesteps | 7804      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -272      |\n",
      "|    critic_loss     | 2.2e+03   |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 7703      |\n",
      "|    reward          | 3.0333571 |\n",
      "----------------------------------\n",
      "======ddpg Validation from:  2016-10-03 to  2017-01-03\n",
      "ddpg Sharpe Ratio:  0.6006980842072044\n",
      "======td3 Training========\n",
      "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.0001}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/td3/td3_378_3\n",
      "day: 1950, episode: 15\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3448018.53\n",
      "total_reward: 2448018.53\n",
      "total_cost: 1075.71\n",
      "total_trades: 25388\n",
      "Sharpe: 1.087\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 87        |\n",
      "|    time_elapsed    | 88        |\n",
      "|    total_timesteps | 7804      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 7.17      |\n",
      "|    critic_loss     | 5.77e+03  |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 7703      |\n",
      "|    reward          | 2.7575684 |\n",
      "----------------------------------\n",
      "======td3 Validation from:  2016-10-03 to  2017-01-03\n",
      "td3 Sharpe Ratio:  0.15096546635311903\n",
      "======sac Training========\n",
      "{'batch_size': 64, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/sac/sac_378_3\n",
      "day: 1950, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2506022.04\n",
      "total_reward: 1506022.04\n",
      "total_cost: 16704.79\n",
      "total_trades: 32481\n",
      "Sharpe: 0.798\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 60        |\n",
      "|    time_elapsed    | 129       |\n",
      "|    total_timesteps | 7804      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.8e+03   |\n",
      "|    critic_loss     | 2.72e+04  |\n",
      "|    ent_coef        | 0.191     |\n",
      "|    ent_coef_loss   | 133       |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 7703      |\n",
      "|    reward          | 2.4233654 |\n",
      "----------------------------------\n",
      "======sac Validation from:  2016-10-03 to  2017-01-03\n",
      "sac Sharpe Ratio:  0.3667396537243817\n",
      "======ppo Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_378_3\n",
      "day: 1950, episode: 25\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2512943.68\n",
      "total_reward: 1512943.68\n",
      "total_cost: 161646.59\n",
      "total_trades: 54958\n",
      "Sharpe: 0.957\n",
      "=================================\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 147        |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 13         |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | -0.3961125 |\n",
      "-----------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 143         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 28          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015599004 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | 0.00761     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 5.71        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0233     |\n",
      "|    reward               | 0.3281864   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 11.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 142         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 43          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015778989 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | -0.00527    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 5.07        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0218     |\n",
      "|    reward               | 0.052194923 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 12.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 141         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 57          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013797167 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | -0.0247     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 4.49        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0152     |\n",
      "|    reward               | 1.0817493   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 12.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 141         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 72          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015470496 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | 0.0197      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 6.75        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0177     |\n",
      "|    reward               | -0.18701233 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 12.4        |\n",
      "-----------------------------------------\n",
      "======ppo Validation from:  2016-10-03 to  2017-01-03\n",
      "ppo Sharpe Ratio:  0.11443966489503021\n",
      "======Best Model Retraining from:  2009-01-01 to  2017-01-03\n",
      "======Trading from:  2017-01-03 to  2017-04-04\n",
      "============================================\n",
      "turbulence_threshold:  160.3155295025199\n",
      "======Model training from:  2009-01-01 to  2017-01-03\n",
      "======a2c Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_441_3\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 127        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | -6.07      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | 48.1       |\n",
      "|    reward             | 0.07736223 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.22       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 127        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0.0704     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -41.6      |\n",
      "|    reward             | -1.6561462 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.12       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 128       |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 11        |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -167      |\n",
      "|    reward             | 4.7605953 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 15.1      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 128        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 15         |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0.0638     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -44.8      |\n",
      "|    reward             | 0.23745938 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.41       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 128         |\n",
      "|    iterations         | 500         |\n",
      "|    time_elapsed       | 19          |\n",
      "|    total_timesteps    | 2500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 499         |\n",
      "|    policy_loss        | 12.2        |\n",
      "|    reward             | -0.16453296 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 1.23        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 128        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 23         |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | 106        |\n",
      "|    reward             | -1.3704292 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 11.1       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 128        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 27         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -8.47      |\n",
      "|    reward             | 0.09196511 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 0.671      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 128       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 31        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0.013     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | 233       |\n",
      "|    reward             | 1.5195894 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 49.5      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 128        |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 34         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | -14.8      |\n",
      "|    reward             | -2.8699968 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 5.02       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 128        |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 38         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | 120        |\n",
      "|    reward             | -0.9854627 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 13.3       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 128      |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 42       |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.2    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 109      |\n",
      "|    reward             | 2.812375 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 19.3     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 128        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 46         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | 395        |\n",
      "|    reward             | -1.5632868 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 153        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 128         |\n",
      "|    iterations         | 1300        |\n",
      "|    time_elapsed       | 50          |\n",
      "|    total_timesteps    | 6500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.2       |\n",
      "|    explained_variance | -0.0587     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1299        |\n",
      "|    policy_loss        | 43.2        |\n",
      "|    reward             | -0.17023396 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 2.3         |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 128        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 54         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | -0.126     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | -121       |\n",
      "|    reward             | -4.5809884 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 11         |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 128      |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 58       |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41      |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | -13.4    |\n",
      "|    reward             | 6.170002 |\n",
      "|    std                | 0.997    |\n",
      "|    value_loss         | 10.7     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 128        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 62         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41        |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | 0.323      |\n",
      "|    reward             | -0.6620876 |\n",
      "|    std                | 0.996      |\n",
      "|    value_loss         | 7.66       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 128        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 65         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | -67.7      |\n",
      "|    reward             | -0.3444471 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 5.52       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 129        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 69         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41        |\n",
      "|    explained_variance | -0.0132    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | -101       |\n",
      "|    reward             | 0.80939734 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 6.6        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 129        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 73         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | -140       |\n",
      "|    reward             | -1.0369776 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 14.1       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 129       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 77        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41       |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 0.505     |\n",
      "|    reward             | 2.2788792 |\n",
      "|    std                | 0.994     |\n",
      "|    value_loss         | 10.5      |\n",
      "-------------------------------------\n",
      "======a2c Validation from:  2017-01-03 to  2017-04-04\n",
      "a2c Sharpe Ratio:  0.05569596907418884\n",
      "======ddpg Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_441_3\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 83         |\n",
      "|    time_elapsed    | 96         |\n",
      "|    total_timesteps | 8056       |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 32.9       |\n",
      "|    critic_loss     | 937        |\n",
      "|    learning_rate   | 0.0005     |\n",
      "|    n_updates       | 7955       |\n",
      "|    reward          | -1.3884779 |\n",
      "-----------------------------------\n",
      "======ddpg Validation from:  2017-01-03 to  2017-04-04\n",
      "ddpg Sharpe Ratio:  0.348465149831086\n",
      "======td3 Training========\n",
      "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.0001}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/td3/td3_441_3\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 86         |\n",
      "|    time_elapsed    | 92         |\n",
      "|    total_timesteps | 8056       |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 6.56       |\n",
      "|    critic_loss     | 1.93e+04   |\n",
      "|    learning_rate   | 0.0001     |\n",
      "|    n_updates       | 7955       |\n",
      "|    reward          | -1.3918905 |\n",
      "-----------------------------------\n",
      "======td3 Validation from:  2017-01-03 to  2017-04-04\n",
      "td3 Sharpe Ratio:  0.30321215431616927\n",
      "======sac Training========\n",
      "{'batch_size': 64, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/sac/sac_441_3\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 60       |\n",
      "|    time_elapsed    | 133      |\n",
      "|    total_timesteps | 8056     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.1e+03  |\n",
      "|    critic_loss     | 78.4     |\n",
      "|    ent_coef        | 0.203    |\n",
      "|    ent_coef_loss   | 203      |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 7955     |\n",
      "|    reward          | -3.28788 |\n",
      "---------------------------------\n",
      "======sac Validation from:  2017-01-03 to  2017-04-04\n",
      "sac Sharpe Ratio:  0.3050019165898199\n",
      "======ppo Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_441_3\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 145         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 14          |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | -0.65862197 |\n",
      "------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 140         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014332272 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.1       |\n",
      "|    explained_variance   | -0.0233     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 8.93        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0216     |\n",
      "|    reward               | 0.26037     |\n",
      "|    std                  | 0.999       |\n",
      "|    value_loss           | 16          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 137         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 44          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019960843 |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.1       |\n",
      "|    explained_variance   | 0.0029      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 3.71        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0169     |\n",
      "|    reward               | 0.9393362   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 10.6        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 137        |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 59         |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.0173961  |\n",
      "|    clip_fraction        | 0.153      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -41.2      |\n",
      "|    explained_variance   | -0.034     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 5.27       |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0213    |\n",
      "|    reward               | 0.34160036 |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 12.5       |\n",
      "----------------------------------------\n",
      "day: 2013, episode: 25\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2584869.08\n",
      "total_reward: 1584869.08\n",
      "total_cost: 160564.31\n",
      "total_trades: 55939\n",
      "Sharpe: 0.831\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 137         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 74          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017548876 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | 0.00517     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 6.68        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.023      |\n",
      "|    reward               | 0.50835603  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 14.4        |\n",
      "-----------------------------------------\n",
      "======ppo Validation from:  2017-01-03 to  2017-04-04\n",
      "ppo Sharpe Ratio:  -0.05473746721312732\n",
      "======Best Model Retraining from:  2009-01-01 to  2017-04-04\n",
      "======Trading from:  2017-04-04 to  2017-07-05\n",
      "============================================\n",
      "turbulence_threshold:  160.3155295025199\n",
      "======Model training from:  2009-01-01 to  2017-04-04\n",
      "======a2c Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_504_2\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 125         |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 3           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41         |\n",
      "|    explained_variance | 0.235       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | 36.2        |\n",
      "|    reward             | -0.01046908 |\n",
      "|    std                | 0.996       |\n",
      "|    value_loss         | 1.19        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 125        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -81.8      |\n",
      "|    reward             | -0.4061251 |\n",
      "|    std                | 0.997      |\n",
      "|    value_loss         | 4.79       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 125       |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 11        |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41       |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -186      |\n",
      "|    reward             | 3.2882307 |\n",
      "|    std                | 0.996     |\n",
      "|    value_loss         | 24.1      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 125      |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 15       |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41      |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -119     |\n",
      "|    reward             | 3.354959 |\n",
      "|    std                | 0.997    |\n",
      "|    value_loss         | 9.92     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 125        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 19         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | 0.0145     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -21        |\n",
      "|    reward             | 0.43589956 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 0.564      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 125        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 23         |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | -90.8      |\n",
      "|    reward             | -1.0754583 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 5.11       |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 125          |\n",
      "|    iterations         | 700          |\n",
      "|    time_elapsed       | 27           |\n",
      "|    total_timesteps    | 3500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -41.1        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 699          |\n",
      "|    policy_loss        | 73.8         |\n",
      "|    reward             | -0.004256614 |\n",
      "|    std                | 0.998        |\n",
      "|    value_loss         | 3.93         |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 125        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 31         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | -79.2      |\n",
      "|    reward             | -1.2042087 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 4.66       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 125        |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 35         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | 69.6       |\n",
      "|    reward             | 0.13071492 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 7.26       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 125        |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 39         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0.00126    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | -134       |\n",
      "|    reward             | -1.8843046 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 10.9       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 125        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 43         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | 7.59       |\n",
      "|    reward             | 0.39119044 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.31       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 125        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 47         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | -56.1      |\n",
      "|    reward             | -1.3140643 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 8.64       |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 125          |\n",
      "|    iterations         | 1300         |\n",
      "|    time_elapsed       | 51           |\n",
      "|    total_timesteps    | 6500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -41.2        |\n",
      "|    explained_variance | 0.155        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1299         |\n",
      "|    policy_loss        | -118         |\n",
      "|    reward             | -0.065395005 |\n",
      "|    std                | 1            |\n",
      "|    value_loss         | 8.15         |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 125        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 55         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0.00971    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | 5.45       |\n",
      "|    reward             | -1.9550529 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 0.415      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 125        |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 59         |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | -23.7      |\n",
      "|    reward             | -1.5888964 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 1.02       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 125        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 63         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | -0.061     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | -356       |\n",
      "|    reward             | -5.0444837 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 101        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 125       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 67        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | -0.466    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 37.4      |\n",
      "|    reward             | 0.4359507 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.6       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 125        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 71         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | -0.149     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | -228       |\n",
      "|    reward             | -2.0892825 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 44         |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 125        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 75         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | 0.0942     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | 55.5       |\n",
      "|    reward             | -1.2024606 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 8.18       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 125       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 79        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 190       |\n",
      "|    reward             | 1.5287074 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 22.9      |\n",
      "-------------------------------------\n",
      "======a2c Validation from:  2017-04-04 to  2017-07-05\n",
      "a2c Sharpe Ratio:  -0.1592072867929214\n",
      "======ddpg Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_504_1\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 82         |\n",
      "|    time_elapsed    | 100        |\n",
      "|    total_timesteps | 8308       |\n",
      "| train/             |            |\n",
      "|    actor_loss      | -22.7      |\n",
      "|    critic_loss     | 31.6       |\n",
      "|    learning_rate   | 0.0005     |\n",
      "|    n_updates       | 8207       |\n",
      "|    reward          | 0.56399816 |\n",
      "-----------------------------------\n",
      "======ddpg Validation from:  2017-04-04 to  2017-07-05\n",
      "ddpg Sharpe Ratio:  0.11883001577748367\n",
      "======td3 Training========\n",
      "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.0001}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/td3/td3_504_1\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    episodes        | 4           |\n",
      "|    fps             | 85          |\n",
      "|    time_elapsed    | 96          |\n",
      "|    total_timesteps | 8308        |\n",
      "| train/             |             |\n",
      "|    actor_loss      | 138         |\n",
      "|    critic_loss     | 461         |\n",
      "|    learning_rate   | 0.0001      |\n",
      "|    n_updates       | 8207        |\n",
      "|    reward          | -0.50631446 |\n",
      "------------------------------------\n",
      "======td3 Validation from:  2017-04-04 to  2017-07-05\n",
      "td3 Sharpe Ratio:  0.1487058023873062\n",
      "======sac Training========\n",
      "{'batch_size': 64, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/sac/sac_504_1\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 59         |\n",
      "|    time_elapsed    | 139        |\n",
      "|    total_timesteps | 8308       |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 691        |\n",
      "|    critic_loss     | 154        |\n",
      "|    ent_coef        | 0.129      |\n",
      "|    ent_coef_loss   | -99.2      |\n",
      "|    learning_rate   | 0.0001     |\n",
      "|    n_updates       | 8207       |\n",
      "|    reward          | -0.6155513 |\n",
      "-----------------------------------\n",
      "======sac Validation from:  2017-04-04 to  2017-07-05\n",
      "sac Sharpe Ratio:  0.14284060257521386\n",
      "======ppo Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_504_1\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 141         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 14          |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | 0.085933425 |\n",
      "------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 138         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013130404 |\n",
      "|    clip_fraction        | 0.238       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | 0.0158      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 5.21        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0204     |\n",
      "|    reward               | -0.32523018 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 14.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 44          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017868955 |\n",
      "|    clip_fraction        | 0.218       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | -0.011      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 6.22        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0216     |\n",
      "|    reward               | -0.8277422  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 14.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 60          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015423668 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | 0.000773    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 5.02        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0207     |\n",
      "|    reward               | -1.4159766  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 11.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 75          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014747481 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | -0.00167    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 4.34        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0176     |\n",
      "|    reward               | 0.4523469   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 13          |\n",
      "-----------------------------------------\n",
      "======ppo Validation from:  2017-04-04 to  2017-07-05\n",
      "ppo Sharpe Ratio:  0.09591249225321272\n",
      "======Best Model Retraining from:  2009-01-01 to  2017-07-05\n",
      "======Trading from:  2017-07-05 to  2017-10-03\n",
      "============================================\n",
      "turbulence_threshold:  160.3155295025199\n",
      "======Model training from:  2009-01-01 to  2017-07-05\n",
      "======a2c Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_567_1\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 123         |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 4           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.3       |\n",
      "|    explained_variance | 0.0183      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -81.5       |\n",
      "|    reward             | -0.06671605 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 6.42        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 124       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0.118     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -27.9     |\n",
      "|    reward             | -1.187747 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 0.656     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 124      |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 12       |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.2    |\n",
      "|    explained_variance | 0.0173   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -153     |\n",
      "|    reward             | 4.580261 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 14.2     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 124         |\n",
      "|    iterations         | 400         |\n",
      "|    time_elapsed       | 16          |\n",
      "|    total_timesteps    | 2000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 399         |\n",
      "|    policy_loss        | -11.9       |\n",
      "|    reward             | -0.21121606 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 12.8        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 124         |\n",
      "|    iterations         | 500         |\n",
      "|    time_elapsed       | 20          |\n",
      "|    total_timesteps    | 2500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.2       |\n",
      "|    explained_variance | 0.182       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 499         |\n",
      "|    policy_loss        | 80          |\n",
      "|    reward             | -0.20105581 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 4.3         |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 124        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 24         |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | 94.3       |\n",
      "|    reward             | -5.6187973 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 9.11       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 124        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 28         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | 84.1       |\n",
      "|    reward             | -1.5367887 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 6.14       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 124       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 32        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -123      |\n",
      "|    reward             | 3.0657554 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 9.4       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 124       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 36        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | -0.0974   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 197       |\n",
      "|    reward             | 0.9721186 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 23.9      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 124        |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 40         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | -52.2      |\n",
      "|    reward             | -4.2556434 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 6.95       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 124       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 44        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | 46.6      |\n",
      "|    reward             | 2.6716144 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 3.88      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 125         |\n",
      "|    iterations         | 1200        |\n",
      "|    time_elapsed       | 47          |\n",
      "|    total_timesteps    | 6000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.2       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1199        |\n",
      "|    policy_loss        | 73.6        |\n",
      "|    reward             | -0.35237992 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 7.89        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 122       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 53        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | -0.0122   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -7.35     |\n",
      "|    reward             | 2.3261812 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.08      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 122       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 57        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -7.3      |\n",
      "|    reward             | 1.6115726 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 0.357     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 122        |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 61         |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | 119        |\n",
      "|    reward             | 0.46510836 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 13.2       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 122       |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 65        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | 74.3      |\n",
      "|    reward             | 0.7318881 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 4.93      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 122       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 69        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | 1.79e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -83.1     |\n",
      "|    reward             | 0.8415445 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 4.59      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 122        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 73         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | 83.2       |\n",
      "|    reward             | 0.84753263 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 5.03       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 122         |\n",
      "|    iterations         | 1900        |\n",
      "|    time_elapsed       | 77          |\n",
      "|    total_timesteps    | 9500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.5       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1899        |\n",
      "|    policy_loss        | 42.5        |\n",
      "|    reward             | -0.37516502 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 5.04        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 122       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 81        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 71        |\n",
      "|    reward             | -1.950748 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 4.35      |\n",
      "-------------------------------------\n",
      "======a2c Validation from:  2017-07-05 to  2017-10-03\n",
      "a2c Sharpe Ratio:  0.22451155730581948\n",
      "======ddpg Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_567_1\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 81        |\n",
      "|    time_elapsed    | 104       |\n",
      "|    total_timesteps | 8560      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -388      |\n",
      "|    critic_loss     | 733       |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 8459      |\n",
      "|    reward          | 0.7339731 |\n",
      "----------------------------------\n",
      "======ddpg Validation from:  2017-07-05 to  2017-10-03\n",
      "ddpg Sharpe Ratio:  0.3718146125720682\n",
      "======td3 Training========\n",
      "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.0001}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/td3/td3_567_1\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 84        |\n",
      "|    time_elapsed    | 101       |\n",
      "|    total_timesteps | 8560      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 99.8      |\n",
      "|    critic_loss     | 193       |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 8459      |\n",
      "|    reward          | 2.3771489 |\n",
      "----------------------------------\n",
      "======td3 Validation from:  2017-07-05 to  2017-10-03\n",
      "td3 Sharpe Ratio:  0.22705511879877754\n",
      "======sac Training========\n",
      "{'batch_size': 64, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/sac/sac_567_1\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 59         |\n",
      "|    time_elapsed    | 143        |\n",
      "|    total_timesteps | 8560       |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 1.95e+03   |\n",
      "|    critic_loss     | 383        |\n",
      "|    ent_coef        | 0.207      |\n",
      "|    ent_coef_loss   | 172        |\n",
      "|    learning_rate   | 0.0001     |\n",
      "|    n_updates       | 8459       |\n",
      "|    reward          | -1.2276007 |\n",
      "-----------------------------------\n",
      "======sac Validation from:  2017-07-05 to  2017-10-03\n",
      "sac Sharpe Ratio:  0.3889284978167772\n",
      "======ppo Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_567_1\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 138         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 14          |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | -0.13211188 |\n",
      "------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 135         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016682807 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | -0.0238     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 5.04        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0204     |\n",
      "|    reward               | 1.6715432   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 12.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 45          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020830533 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | -0.0371     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 4.33        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0214     |\n",
      "|    reward               | -0.9964169  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 11.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 61          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018863207 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | -0.019      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 5.12        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0191     |\n",
      "|    reward               | 0.6015777   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 9.99        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 133         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 76          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020368066 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | 0.0152      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 5.53        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0211     |\n",
      "|    reward               | -3.9899576  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 10.3        |\n",
      "-----------------------------------------\n",
      "======ppo Validation from:  2017-07-05 to  2017-10-03\n",
      "ppo Sharpe Ratio:  0.3093038467550866\n",
      "======Best Model Retraining from:  2009-01-01 to  2017-10-03\n",
      "======Trading from:  2017-10-03 to  2018-01-03\n",
      "============================================\n",
      "turbulence_threshold:  160.3155295025199\n",
      "======Model training from:  2009-01-01 to  2017-10-03\n",
      "======a2c Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_630_1\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | -1.97      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | 9.8        |\n",
      "|    reward             | 0.19440864 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 0.358      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 8          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | -0.826     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -40        |\n",
      "|    reward             | -2.6858885 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 1.35       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 121      |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 12       |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.1    |\n",
      "|    explained_variance | -0.0637  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -340     |\n",
      "|    reward             | 5.353567 |\n",
      "|    std                | 0.998    |\n",
      "|    value_loss         | 73.5     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 16        |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41       |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | 116       |\n",
      "|    reward             | 1.5234987 |\n",
      "|    std                | 0.996     |\n",
      "|    value_loss         | 10.5      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 121         |\n",
      "|    iterations         | 500         |\n",
      "|    time_elapsed       | 20          |\n",
      "|    total_timesteps    | 2500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41         |\n",
      "|    explained_variance | -0.452      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 499         |\n",
      "|    policy_loss        | 33.1        |\n",
      "|    reward             | -0.26665953 |\n",
      "|    std                | 0.997       |\n",
      "|    value_loss         | 1.14        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 121         |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 24          |\n",
      "|    total_timesteps    | 3000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.1       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | 91.8        |\n",
      "|    reward             | -0.48345053 |\n",
      "|    std                | 0.997       |\n",
      "|    value_loss         | 5.85        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 28         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -104       |\n",
      "|    reward             | 0.23229955 |\n",
      "|    std                | 0.996      |\n",
      "|    value_loss         | 8.6        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 32         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | 1.79e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | 287        |\n",
      "|    reward             | 0.67399967 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 54.5       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 121         |\n",
      "|    iterations         | 900         |\n",
      "|    time_elapsed       | 36          |\n",
      "|    total_timesteps    | 4500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.1       |\n",
      "|    explained_variance | 0.0496      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 899         |\n",
      "|    policy_loss        | -25.3       |\n",
      "|    reward             | -0.37839797 |\n",
      "|    std                | 0.997       |\n",
      "|    value_loss         | 1.12        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 41        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -126      |\n",
      "|    reward             | 1.7259698 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 8.73      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 45         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41        |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | 20.3       |\n",
      "|    reward             | -1.8005534 |\n",
      "|    std                | 0.997      |\n",
      "|    value_loss         | 1.28       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 121         |\n",
      "|    iterations         | 1200        |\n",
      "|    time_elapsed       | 49          |\n",
      "|    total_timesteps    | 6000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41         |\n",
      "|    explained_variance | 0.049       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1199        |\n",
      "|    policy_loss        | -28.6       |\n",
      "|    reward             | -0.77574116 |\n",
      "|    std                | 0.997       |\n",
      "|    value_loss         | 2.58        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 53         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 28.5       |\n",
      "|    reward             | 0.10690492 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 1.85       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 57        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -63.3     |\n",
      "|    reward             | 1.0192428 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 4.09      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 122      |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 61       |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41      |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | -83.9    |\n",
      "|    reward             | 1.726385 |\n",
      "|    std                | 0.997    |\n",
      "|    value_loss         | 5.24     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 122         |\n",
      "|    iterations         | 1600        |\n",
      "|    time_elapsed       | 65          |\n",
      "|    total_timesteps    | 8000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41         |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1599        |\n",
      "|    policy_loss        | -216        |\n",
      "|    reward             | -0.41558713 |\n",
      "|    std                | 0.997       |\n",
      "|    value_loss         | 31.3        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 122       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 69        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 227       |\n",
      "|    reward             | 4.2752314 |\n",
      "|    std                | 0.996     |\n",
      "|    value_loss         | 72.7      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 122        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 73         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | -0.486     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | -69.5      |\n",
      "|    reward             | -1.6069868 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 3.31       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 122      |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 77       |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41      |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 97       |\n",
      "|    reward             | 4.394887 |\n",
      "|    std                | 0.997    |\n",
      "|    value_loss         | 12.1     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 122        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 81         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | 192        |\n",
      "|    reward             | 0.75895417 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 26         |\n",
      "--------------------------------------\n",
      "======a2c Validation from:  2017-10-03 to  2018-01-03\n",
      "a2c Sharpe Ratio:  0.5719261035123766\n",
      "======ddpg Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_630_1\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 79        |\n",
      "|    time_elapsed    | 110       |\n",
      "|    total_timesteps | 8812      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 119       |\n",
      "|    critic_loss     | 557       |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 8711      |\n",
      "|    reward          | 2.8123834 |\n",
      "----------------------------------\n",
      "======ddpg Validation from:  2017-10-03 to  2018-01-03\n",
      "ddpg Sharpe Ratio:  0.3455541403326951\n",
      "======td3 Training========\n",
      "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.0001}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/td3/td3_630_1\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 83       |\n",
      "|    time_elapsed    | 105      |\n",
      "|    total_timesteps | 8812     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 179      |\n",
      "|    critic_loss     | 298      |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 8711     |\n",
      "|    reward          | 2.313679 |\n",
      "---------------------------------\n",
      "======td3 Validation from:  2017-10-03 to  2018-01-03\n",
      "td3 Sharpe Ratio:  0.6479317241299586\n",
      "======sac Training========\n",
      "{'batch_size': 64, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/sac/sac_630_1\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 58        |\n",
      "|    time_elapsed    | 149       |\n",
      "|    total_timesteps | 8812      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.73e+03  |\n",
      "|    critic_loss     | 5.55e+04  |\n",
      "|    ent_coef        | 0.21      |\n",
      "|    ent_coef_loss   | 157       |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 8711      |\n",
      "|    reward          | 2.7482166 |\n",
      "----------------------------------\n",
      "======sac Validation from:  2017-10-03 to  2018-01-03\n",
      "sac Sharpe Ratio:  0.7379425906314859\n",
      "======ppo Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_630_1\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 135         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 15          |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | -0.14154051 |\n",
      "------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 132        |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 30         |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02161637 |\n",
      "|    clip_fraction        | 0.204      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -41.2      |\n",
      "|    explained_variance   | -0.0304    |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 6.34       |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.0238    |\n",
      "|    reward               | 1.6873711  |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 12.7       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 131         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 46          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016785353 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | -0.0181     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 6.46        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0188     |\n",
      "|    reward               | 0.18809468  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 12.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 131         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 62          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015593809 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.4       |\n",
      "|    explained_variance   | 0.00945     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 4.62        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0216     |\n",
      "|    reward               | 2.4947133   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 12.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 131         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 78          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014730906 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.4       |\n",
      "|    explained_variance   | 0.0262      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 7.36        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0195     |\n",
      "|    reward               | 1.4107361   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 13.8        |\n",
      "-----------------------------------------\n",
      "======ppo Validation from:  2017-10-03 to  2018-01-03\n",
      "ppo Sharpe Ratio:  0.46969414005984933\n",
      "======Best Model Retraining from:  2009-01-01 to  2018-01-03\n",
      "======Trading from:  2018-01-03 to  2018-04-05\n",
      "============================================\n",
      "turbulence_threshold:  160.3155295025199\n",
      "======Model training from:  2009-01-01 to  2018-01-03\n",
      "======a2c Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_693_1\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 119          |\n",
      "|    iterations         | 100          |\n",
      "|    time_elapsed       | 4            |\n",
      "|    total_timesteps    | 500          |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -41.3        |\n",
      "|    explained_variance | -0.632       |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 99           |\n",
      "|    policy_loss        | -125         |\n",
      "|    reward             | -0.039498206 |\n",
      "|    std                | 1.01         |\n",
      "|    value_loss         | 10.2         |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 119         |\n",
      "|    iterations         | 200         |\n",
      "|    time_elapsed       | 8           |\n",
      "|    total_timesteps    | 1000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.4       |\n",
      "|    explained_variance | -0.1        |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 199         |\n",
      "|    policy_loss        | -44.6       |\n",
      "|    reward             | -0.61859804 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 2.4         |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 120      |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 12       |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.4    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -150     |\n",
      "|    reward             | 4.963571 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 14.4     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 119       |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 16        |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | 30.4      |\n",
      "|    reward             | 0.1622052 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.53      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 119         |\n",
      "|    iterations         | 500         |\n",
      "|    time_elapsed       | 20          |\n",
      "|    total_timesteps    | 2500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.3       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 499         |\n",
      "|    policy_loss        | -0.592      |\n",
      "|    reward             | -0.24071854 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 1.73        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 119      |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 25       |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.3    |\n",
      "|    explained_variance | 0.00368  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | -278     |\n",
      "|    reward             | 9.729682 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 66.5     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 119       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 29        |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 5.3       |\n",
      "|    reward             | 1.0517253 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.6       |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 119          |\n",
      "|    iterations         | 800          |\n",
      "|    time_elapsed       | 33           |\n",
      "|    total_timesteps    | 4000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -41.3        |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 799          |\n",
      "|    policy_loss        | -403         |\n",
      "|    reward             | -0.065228276 |\n",
      "|    std                | 1            |\n",
      "|    value_loss         | 92.6         |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 119        |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 37         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | -72.4      |\n",
      "|    reward             | -1.8092952 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 5.43       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 119         |\n",
      "|    iterations         | 1000        |\n",
      "|    time_elapsed       | 41          |\n",
      "|    total_timesteps    | 5000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 999         |\n",
      "|    policy_loss        | 41.2        |\n",
      "|    reward             | -0.29988465 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 1.45        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 119       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 45        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -99.5     |\n",
      "|    reward             | 3.0626156 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 14.1      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 119      |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 50       |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.2    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 386      |\n",
      "|    reward             | 4.075477 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 95.5     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 119        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 54         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0.00589    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 102        |\n",
      "|    reward             | 0.30217472 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 6.77       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 119        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 58         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | 76.4       |\n",
      "|    reward             | -1.4632314 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 4.59       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 119       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 62        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 35.1      |\n",
      "|    reward             | 1.3704227 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 9.18      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 119      |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 66       |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.3    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | -61      |\n",
      "|    reward             | 2.227169 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 5.5      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 120      |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 70       |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.3    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 11.2     |\n",
      "|    reward             | 3.368048 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 3.16     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 120      |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 74       |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.3    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 110      |\n",
      "|    reward             | 2.049062 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 10.2     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 120        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 79         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | 233        |\n",
      "|    reward             | -1.6451315 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 32.3       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 120         |\n",
      "|    iterations         | 2000        |\n",
      "|    time_elapsed       | 83          |\n",
      "|    total_timesteps    | 10000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.4       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1999        |\n",
      "|    policy_loss        | 164         |\n",
      "|    reward             | -0.33036524 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 15.7        |\n",
      "---------------------------------------\n",
      "======a2c Validation from:  2018-01-03 to  2018-04-05\n",
      "a2c Sharpe Ratio:  -0.06826800795594376\n",
      "======ddpg Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_693_1\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 79        |\n",
      "|    time_elapsed    | 113       |\n",
      "|    total_timesteps | 9064      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -312      |\n",
      "|    critic_loss     | 306       |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 8963      |\n",
      "|    reward          | 1.0646789 |\n",
      "----------------------------------\n",
      "======ddpg Validation from:  2018-01-03 to  2018-04-05\n",
      "ddpg Sharpe Ratio:  -0.06449202458168952\n",
      "======td3 Training========\n",
      "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.0001}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/td3/td3_693_1\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 81        |\n",
      "|    time_elapsed    | 111       |\n",
      "|    total_timesteps | 9064      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 288       |\n",
      "|    critic_loss     | 441       |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 8963      |\n",
      "|    reward          | 2.6714404 |\n",
      "----------------------------------\n",
      "======td3 Validation from:  2018-01-03 to  2018-04-05\n",
      "td3 Sharpe Ratio:  -0.08601972659316615\n",
      "======sac Training========\n",
      "{'batch_size': 64, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/sac/sac_693_1\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    episodes        | 4           |\n",
      "|    fps             | 58          |\n",
      "|    time_elapsed    | 156         |\n",
      "|    total_timesteps | 9064        |\n",
      "| train/             |             |\n",
      "|    actor_loss      | 1.56e+03    |\n",
      "|    critic_loss     | 317         |\n",
      "|    ent_coef        | 0.216       |\n",
      "|    ent_coef_loss   | 192         |\n",
      "|    learning_rate   | 0.0001      |\n",
      "|    n_updates       | 8963        |\n",
      "|    reward          | -0.44581994 |\n",
      "------------------------------------\n",
      "======sac Validation from:  2018-01-03 to  2018-04-05\n",
      "sac Sharpe Ratio:  -0.059679646150501725\n",
      "======ppo Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_693_1\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    fps             | 133       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 15        |\n",
      "|    total_timesteps | 2048      |\n",
      "| train/             |           |\n",
      "|    reward          | 0.4993664 |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 130         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020185608 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | -0.0168     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 3.86        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0194     |\n",
      "|    reward               | 2.1171303   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 11.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 129         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 47          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016272418 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | 0.000823    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 6.54        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0225     |\n",
      "|    reward               | 0.7651866   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 11.6        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 129        |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 63         |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01803803 |\n",
      "|    clip_fraction        | 0.167      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -41.3      |\n",
      "|    explained_variance   | 0.0166     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 3.19       |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0295    |\n",
      "|    reward               | 1.3971765  |\n",
      "|    std                  | 1.01       |\n",
      "|    value_loss           | 11.3       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 128          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 79           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.015871499  |\n",
      "|    clip_fraction        | 0.202        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -41.3        |\n",
      "|    explained_variance   | -0.0303      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 8.91         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0252      |\n",
      "|    reward               | -0.010170147 |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 18.4         |\n",
      "------------------------------------------\n",
      "======ppo Validation from:  2018-01-03 to  2018-04-05\n",
      "ppo Sharpe Ratio:  -0.22135984047218343\n",
      "======Best Model Retraining from:  2009-01-01 to  2018-04-05\n",
      "======Trading from:  2018-04-05 to  2018-07-05\n",
      "============================================\n",
      "turbulence_threshold:  160.3155295025199\n",
      "======Model training from:  2009-01-01 to  2018-04-05\n",
      "======a2c Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_756_1\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 116          |\n",
      "|    iterations         | 100          |\n",
      "|    time_elapsed       | 4            |\n",
      "|    total_timesteps    | 500          |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -41.2        |\n",
      "|    explained_variance | -0.0336      |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 99           |\n",
      "|    policy_loss        | 5.05         |\n",
      "|    reward             | -0.007046619 |\n",
      "|    std                | 1            |\n",
      "|    value_loss         | 0.335        |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 117       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -29       |\n",
      "|    reward             | -1.952762 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.04      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 117       |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 12        |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -379      |\n",
      "|    reward             | 5.1989346 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 84.3      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 117        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 17         |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 1.79e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -10.4      |\n",
      "|    reward             | -0.1899988 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 18.4       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 117       |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 21        |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | -51.8     |\n",
      "|    reward             | 0.2177815 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.79      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 117       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 25        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | -0.0968   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | 150       |\n",
      "|    reward             | 0.6431754 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 21.8      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 117        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 29         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -108       |\n",
      "|    reward             | -0.9580849 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 11.3       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 117        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 34         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | 116        |\n",
      "|    reward             | -14.896989 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 9.23       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 117       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 38        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -203      |\n",
      "|    reward             | -4.821432 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 24.9      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 117        |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 42         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | -227       |\n",
      "|    reward             | -1.0970944 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 36.1       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 117       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 46        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -77.8     |\n",
      "|    reward             | 1.1808064 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 3.99      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 117         |\n",
      "|    iterations         | 1200        |\n",
      "|    time_elapsed       | 50          |\n",
      "|    total_timesteps    | 6000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.3       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1199        |\n",
      "|    policy_loss        | 69.2        |\n",
      "|    reward             | -0.13863009 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 3.43        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 117        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 55         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 42.9       |\n",
      "|    reward             | -3.6569679 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 11.1       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 117         |\n",
      "|    iterations         | 1400        |\n",
      "|    time_elapsed       | 59          |\n",
      "|    total_timesteps    | 7000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1399        |\n",
      "|    policy_loss        | -28.8       |\n",
      "|    reward             | -0.14452282 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 0.564       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 117       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 63        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 103       |\n",
      "|    reward             | 1.3127564 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 9.72      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 117        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 67         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | -40.8      |\n",
      "|    reward             | -0.5417473 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 3.79       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 117       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 72        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -50.8     |\n",
      "|    reward             | 5.5144734 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.86      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 117       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 76        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -77.6     |\n",
      "|    reward             | -2.240382 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 7.23      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 117       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 80        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | -0.0242   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | 3.56      |\n",
      "|    reward             | -1.279237 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 0.249     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 117         |\n",
      "|    iterations         | 2000        |\n",
      "|    time_elapsed       | 84          |\n",
      "|    total_timesteps    | 10000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.4       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1999        |\n",
      "|    policy_loss        | -10         |\n",
      "|    reward             | -0.75377303 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 4.06        |\n",
      "---------------------------------------\n",
      "======a2c Validation from:  2018-04-05 to  2018-07-05\n",
      "a2c Sharpe Ratio:  0.1548235093945528\n",
      "======ddpg Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_756_1\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 78       |\n",
      "|    time_elapsed    | 118      |\n",
      "|    total_timesteps | 9316     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18      |\n",
      "|    critic_loss     | 4.99     |\n",
      "|    learning_rate   | 0.0005   |\n",
      "|    n_updates       | 9215     |\n",
      "|    reward          | 5.83667  |\n",
      "---------------------------------\n",
      "======ddpg Validation from:  2018-04-05 to  2018-07-05\n",
      "ddpg Sharpe Ratio:  0.004537179330046326\n",
      "======td3 Training========\n",
      "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.0001}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/td3/td3_756_1\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 81         |\n",
      "|    time_elapsed    | 114        |\n",
      "|    total_timesteps | 9316       |\n",
      "| train/             |            |\n",
      "|    actor_loss      | -66.4      |\n",
      "|    critic_loss     | 972        |\n",
      "|    learning_rate   | 0.0001     |\n",
      "|    n_updates       | 9215       |\n",
      "|    reward          | 0.92685604 |\n",
      "-----------------------------------\n",
      "======td3 Validation from:  2018-04-05 to  2018-07-05\n",
      "td3 Sharpe Ratio:  0.17921777494783486\n",
      "======sac Training========\n",
      "{'batch_size': 64, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/sac/sac_756_1\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    episodes        | 4           |\n",
      "|    fps             | 57          |\n",
      "|    time_elapsed    | 160         |\n",
      "|    total_timesteps | 9316        |\n",
      "| train/             |             |\n",
      "|    actor_loss      | 804         |\n",
      "|    critic_loss     | 1.54e+04    |\n",
      "|    ent_coef        | 0.159       |\n",
      "|    ent_coef_loss   | -69         |\n",
      "|    learning_rate   | 0.0001      |\n",
      "|    n_updates       | 9215        |\n",
      "|    reward          | -0.26711768 |\n",
      "------------------------------------\n",
      "======sac Validation from:  2018-04-05 to  2018-07-05\n",
      "sac Sharpe Ratio:  -0.06663091075139771\n",
      "======ppo Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_756_1\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 130          |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 15           |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | -0.076325186 |\n",
      "-------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 32          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013402509 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | -0.000521   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 7.27        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0274     |\n",
      "|    reward               | 0.18567148  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 13          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 48          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017655272 |\n",
      "|    clip_fraction        | 0.219       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | -0.00476    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 8.7         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0246     |\n",
      "|    reward               | 0.6924667   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 15.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 64          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020359118 |\n",
      "|    clip_fraction        | 0.226       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | 0.00943     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 3.34        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0261     |\n",
      "|    reward               | 4.321576    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 11.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 81          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018030174 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.4       |\n",
      "|    explained_variance   | 0.0194      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 7.48        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0222     |\n",
      "|    reward               | 0.90964997  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 20.2        |\n",
      "-----------------------------------------\n",
      "======ppo Validation from:  2018-04-05 to  2018-07-05\n",
      "ppo Sharpe Ratio:  -0.08611211345865682\n",
      "======Best Model Retraining from:  2009-01-01 to  2018-07-05\n",
      "======Trading from:  2018-07-05 to  2018-10-03\n",
      "============================================\n",
      "turbulence_threshold:  160.3155295025199\n",
      "======Model training from:  2009-01-01 to  2018-07-05\n",
      "======a2c Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_819_1\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 115        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.5      |\n",
      "|    explained_variance | 0.0139     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | 23.7       |\n",
      "|    reward             | -0.2730829 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.539      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 115        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 8          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -66.6      |\n",
      "|    reward             | -1.3625766 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 3.77       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 115       |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 12        |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -233      |\n",
      "|    reward             | 7.6294036 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 37.3      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 115        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 17         |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.5      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -34.3      |\n",
      "|    reward             | -0.8438313 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 17.5       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 115       |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 21        |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 0.131     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | 84.5      |\n",
      "|    reward             | 0.3374923 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 5.69      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 115        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 26         |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | -162       |\n",
      "|    reward             | 0.85901654 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 21.5       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 115       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 30        |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -138      |\n",
      "|    reward             | 1.8059179 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 13        |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 115        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 34         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | 242        |\n",
      "|    reward             | -3.4193785 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 47.5       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 115      |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 38       |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.4    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | -100     |\n",
      "|    reward             | 6.664815 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 7.2      |\n",
      "------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 115          |\n",
      "|    iterations         | 1000         |\n",
      "|    time_elapsed       | 43           |\n",
      "|    total_timesteps    | 5000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -41.4        |\n",
      "|    explained_variance | -0.0361      |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 999          |\n",
      "|    policy_loss        | -47.5        |\n",
      "|    reward             | -0.011744221 |\n",
      "|    std                | 1.01         |\n",
      "|    value_loss         | 6.81         |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 115       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 47        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | 139       |\n",
      "|    reward             | 0.6682671 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 14.2      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 115       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 51        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -139      |\n",
      "|    reward             | 0.5511831 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 14.5      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 116      |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 56       |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.3    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 21.6     |\n",
      "|    reward             | 4.881263 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 6.03     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 116      |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 60       |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.3    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | -335     |\n",
      "|    reward             | 7.319019 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 88.2     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 116       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 64        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 1.41      |\n",
      "|    reward             | 1.5218291 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.311     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 116       |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 68        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | 95.3      |\n",
      "|    reward             | 1.9527601 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 7.62      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 116       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 73        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 40.5      |\n",
      "|    reward             | 1.9772843 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.49      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 116       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 77        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 24.7      |\n",
      "|    reward             | 2.1315012 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.87      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 116      |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 81       |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.4    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | -277     |\n",
      "|    reward             | 4.601025 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 47.5     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 116       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 86        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 60        |\n",
      "|    reward             | 0.2520431 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 3.32      |\n",
      "-------------------------------------\n",
      "======a2c Validation from:  2018-07-05 to  2018-10-03\n",
      "a2c Sharpe Ratio:  0.41252934529368185\n",
      "======ddpg Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_819_1\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 77        |\n",
      "|    time_elapsed    | 123       |\n",
      "|    total_timesteps | 9568      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -67.9     |\n",
      "|    critic_loss     | 3.83      |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 9467      |\n",
      "|    reward          | -2.661863 |\n",
      "----------------------------------\n",
      "======ddpg Validation from:  2018-07-05 to  2018-10-03\n",
      "ddpg Sharpe Ratio:  0.4860381339632303\n",
      "======td3 Training========\n",
      "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.0001}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/td3/td3_819_1\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 80         |\n",
      "|    time_elapsed    | 119        |\n",
      "|    total_timesteps | 9568       |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 19.9       |\n",
      "|    critic_loss     | 634        |\n",
      "|    learning_rate   | 0.0001     |\n",
      "|    n_updates       | 9467       |\n",
      "|    reward          | -1.9304228 |\n",
      "-----------------------------------\n",
      "======td3 Validation from:  2018-07-05 to  2018-10-03\n",
      "td3 Sharpe Ratio:  0.6269455626554813\n",
      "======sac Training========\n",
      "{'batch_size': 64, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/sac/sac_819_1\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 57        |\n",
      "|    time_elapsed    | 166       |\n",
      "|    total_timesteps | 9568      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 2.89e+03  |\n",
      "|    critic_loss     | 7.07e+03  |\n",
      "|    ent_coef        | 0.251     |\n",
      "|    ent_coef_loss   | 472       |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 9467      |\n",
      "|    reward          | -4.882776 |\n",
      "----------------------------------\n",
      "======sac Validation from:  2018-07-05 to  2018-10-03\n",
      "sac Sharpe Ratio:  0.5417748623051952\n",
      "======ppo Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_819_1\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 128          |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 15           |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | -0.010582138 |\n",
      "-------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 125          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 32           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0149766775 |\n",
      "|    clip_fraction        | 0.213        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -41.2        |\n",
      "|    explained_variance   | 0.00916      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 7.53         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0235      |\n",
      "|    reward               | 1.2236518    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 14.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 49          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018274553 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | 0.00732     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 4.62        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0169     |\n",
      "|    reward               | -0.29033288 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 15.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 65          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024128743 |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | -0.0342     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 5.55        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0208     |\n",
      "|    reward               | -0.8638352  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 12.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 82          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016094072 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | -0.0176     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 11.9        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.021      |\n",
      "|    reward               | -1.8473175  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 18.7        |\n",
      "-----------------------------------------\n",
      "======ppo Validation from:  2018-07-05 to  2018-10-03\n",
      "ppo Sharpe Ratio:  0.39144215102460783\n",
      "======Best Model Retraining from:  2009-01-01 to  2018-10-03\n",
      "======Trading from:  2018-10-03 to  2019-01-04\n",
      "============================================\n",
      "turbulence_threshold:  160.3155295025199\n",
      "======Model training from:  2009-01-01 to  2018-10-03\n",
      "======a2c Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_882_1\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 114         |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 4           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.3       |\n",
      "|    explained_variance | -0.543      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -14         |\n",
      "|    reward             | 0.028782895 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 0.245       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 114        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 8          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | -0.707     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -77.2      |\n",
      "|    reward             | -1.1798863 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 3.82       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 114       |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -187      |\n",
      "|    reward             | 7.5455866 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 21.4      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 114       |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 17        |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -120      |\n",
      "|    reward             | 0.7029102 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 13.1      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 114       |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 21        |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 2.38e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | -75.9     |\n",
      "|    reward             | 1.7472475 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 4.8       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 114      |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 26       |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.3    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | -71.6    |\n",
      "|    reward             | 3.557226 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 3.35     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 114        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 30         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -63.4      |\n",
      "|    reward             | -1.2691092 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 4.23       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 114       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 34        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | 69.2      |\n",
      "|    reward             | -1.202885 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 10.7      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 114         |\n",
      "|    iterations         | 900         |\n",
      "|    time_elapsed       | 39          |\n",
      "|    total_timesteps    | 4500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.2       |\n",
      "|    explained_variance | -0.028      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 899         |\n",
      "|    policy_loss        | 99.6        |\n",
      "|    reward             | -0.47977448 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 7.82        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 111        |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 44         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | 75.1       |\n",
      "|    reward             | -1.2166677 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 4.78       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 111       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 49        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | 86.5      |\n",
      "|    reward             | 0.6095467 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 6.97      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 111       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 53        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0.028     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 26        |\n",
      "|    reward             | 1.4567381 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 2.33      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 111       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 58        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0.0905    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | 81.5      |\n",
      "|    reward             | -0.803278 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 5.68      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 111         |\n",
      "|    iterations         | 1400        |\n",
      "|    time_elapsed       | 62          |\n",
      "|    total_timesteps    | 7000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.3       |\n",
      "|    explained_variance | -0.0515     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1399        |\n",
      "|    policy_loss        | 53.5        |\n",
      "|    reward             | -0.45429578 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 2.11        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 111       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 66        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -27.9     |\n",
      "|    reward             | 0.9253837 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.667     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 112         |\n",
      "|    iterations         | 1600        |\n",
      "|    time_elapsed       | 71          |\n",
      "|    total_timesteps    | 8000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.3       |\n",
      "|    explained_variance | 0.00915     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1599        |\n",
      "|    policy_loss        | 174         |\n",
      "|    reward             | -0.07684684 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 23.8        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 112       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 75        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0.0145    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 41.6      |\n",
      "|    reward             | 1.0813239 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 6.68      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 112       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 80        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 29.2      |\n",
      "|    reward             | -2.429516 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 7.03      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 112        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 84         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | 108        |\n",
      "|    reward             | -3.1069603 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 11         |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 112        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 88         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | 40.3       |\n",
      "|    reward             | -0.7739447 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 2.62       |\n",
      "--------------------------------------\n",
      "======a2c Validation from:  2018-10-03 to  2019-01-04\n",
      "a2c Sharpe Ratio:  -0.1111127116132733\n",
      "======ddpg Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_882_1\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 76        |\n",
      "|    time_elapsed    | 128       |\n",
      "|    total_timesteps | 9820      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 24.2      |\n",
      "|    critic_loss     | 19.4      |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 9719      |\n",
      "|    reward          | 1.0983394 |\n",
      "----------------------------------\n",
      "======ddpg Validation from:  2018-10-03 to  2019-01-04\n",
      "ddpg Sharpe Ratio:  -0.16248057503169736\n",
      "======td3 Training========\n",
      "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.0001}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/td3/td3_882_1\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 79        |\n",
      "|    time_elapsed    | 123       |\n",
      "|    total_timesteps | 9820      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 46.9      |\n",
      "|    critic_loss     | 8.04      |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 9719      |\n",
      "|    reward          | 1.1499438 |\n",
      "----------------------------------\n",
      "======td3 Validation from:  2018-10-03 to  2019-01-04\n",
      "td3 Sharpe Ratio:  -0.09113351191825933\n",
      "======sac Training========\n",
      "{'batch_size': 64, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/sac/sac_882_1\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 56        |\n",
      "|    time_elapsed    | 172       |\n",
      "|    total_timesteps | 9820      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.77e+03  |\n",
      "|    critic_loss     | 172       |\n",
      "|    ent_coef        | 0.23      |\n",
      "|    ent_coef_loss   | 175       |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 9719      |\n",
      "|    reward          | 1.4197892 |\n",
      "----------------------------------\n",
      "======sac Validation from:  2018-10-03 to  2019-01-04\n",
      "sac Sharpe Ratio:  -0.11374744258120612\n",
      "======ppo Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_882_1\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 126        |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 16         |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.20108713 |\n",
      "-----------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 33          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012831446 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | 0.0062      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 6.63        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.02       |\n",
      "|    reward               | 2.3963902   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 12.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 50          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017262926 |\n",
      "|    clip_fraction        | 0.229       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | -0.0225     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 7.56        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0236     |\n",
      "|    reward               | -0.76942414 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 15          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 66          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019577518 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | -0.0336     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 8.66        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0185     |\n",
      "|    reward               | 0.40878528  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 15.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 83          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013144249 |\n",
      "|    clip_fraction        | 0.166       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | -0.0624     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 10.4        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0196     |\n",
      "|    reward               | 0.5260812   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 16.9        |\n",
      "-----------------------------------------\n",
      "======ppo Validation from:  2018-10-03 to  2019-01-04\n",
      "ppo Sharpe Ratio:  -0.29813776342647663\n",
      "======Best Model Retraining from:  2009-01-01 to  2019-01-04\n",
      "======Trading from:  2019-01-04 to  2019-04-05\n",
      "============================================\n",
      "turbulence_threshold:  160.3155295025199\n",
      "======Model training from:  2009-01-01 to  2019-01-04\n",
      "======a2c Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_945_1\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 111        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41        |\n",
      "|    explained_variance | 0.0617     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | 8.91       |\n",
      "|    reward             | 0.27800712 |\n",
      "|    std                | 0.996      |\n",
      "|    value_loss         | 0.404      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 111       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -74.9     |\n",
      "|    reward             | 3.3691428 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 3.91      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 111      |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 13       |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.1    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -261     |\n",
      "|    reward             | 7.048425 |\n",
      "|    std                | 0.997    |\n",
      "|    value_loss         | 47.4     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 111        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 17         |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | 111        |\n",
      "|    reward             | -1.0240525 |\n",
      "|    std                | 0.997      |\n",
      "|    value_loss         | 17.5       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 111        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 22         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 362        |\n",
      "|    reward             | -10.658659 |\n",
      "|    std                | 0.996      |\n",
      "|    value_loss         | 127        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 111       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 26        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41       |\n",
      "|    explained_variance | -0.0258   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -10.9     |\n",
      "|    reward             | 3.6062927 |\n",
      "|    std                | 0.996     |\n",
      "|    value_loss         | 3.26      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 111        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 31         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41        |\n",
      "|    explained_variance | -0.191     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | 182        |\n",
      "|    reward             | -0.3511713 |\n",
      "|    std                | 0.996      |\n",
      "|    value_loss         | 19.1       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 111       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 35        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41       |\n",
      "|    explained_variance | 0.0953    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | 44.7      |\n",
      "|    reward             | 0.2846246 |\n",
      "|    std                | 0.994     |\n",
      "|    value_loss         | 1.73      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 111       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 40        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41       |\n",
      "|    explained_variance | 0.0984    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 222       |\n",
      "|    reward             | 2.9187841 |\n",
      "|    std                | 0.994     |\n",
      "|    value_loss         | 35.9      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 111         |\n",
      "|    iterations         | 1000        |\n",
      "|    time_elapsed       | 44          |\n",
      "|    total_timesteps    | 5000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41         |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 999         |\n",
      "|    policy_loss        | 234         |\n",
      "|    reward             | -0.30134442 |\n",
      "|    std                | 0.994       |\n",
      "|    value_loss         | 62.5        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 111       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 49        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -115      |\n",
      "|    reward             | 2.7617388 |\n",
      "|    std                | 0.995     |\n",
      "|    value_loss         | 7.49      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 112       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 53        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41       |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -10.7     |\n",
      "|    reward             | 1.6478709 |\n",
      "|    std                | 0.994     |\n",
      "|    value_loss         | 0.401     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 112        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 57         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 125        |\n",
      "|    reward             | 0.91873616 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 13.4       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 112       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 62        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -184      |\n",
      "|    reward             | 6.9646177 |\n",
      "|    std                | 0.995     |\n",
      "|    value_loss         | 36.2      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 112      |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 66       |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.1    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | -265     |\n",
      "|    reward             | 9.404882 |\n",
      "|    std                | 0.998    |\n",
      "|    value_loss         | 77.5     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 112         |\n",
      "|    iterations         | 1600        |\n",
      "|    time_elapsed       | 71          |\n",
      "|    total_timesteps    | 8000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.1       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1599        |\n",
      "|    policy_loss        | -22.6       |\n",
      "|    reward             | -0.47997543 |\n",
      "|    std                | 0.998       |\n",
      "|    value_loss         | 1.13        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 112       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 75        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -32.7     |\n",
      "|    reward             | 0.8836474 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 0.968     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 112        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 80         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | -35.3      |\n",
      "|    reward             | -2.2578995 |\n",
      "|    std                | 0.997      |\n",
      "|    value_loss         | 11.6       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 112       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 84        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | 0.00302   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -69.5     |\n",
      "|    reward             | -5.072102 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 14        |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 112       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 89        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 232       |\n",
      "|    reward             | 6.2282577 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 50        |\n",
      "-------------------------------------\n",
      "======a2c Validation from:  2019-01-04 to  2019-04-05\n",
      "a2c Sharpe Ratio:  0.31146261070340675\n",
      "======ddpg Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_945_1\n",
      "day: 2517, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2921190.04\n",
      "total_reward: 1921190.04\n",
      "total_cost: 4718.69\n",
      "total_trades: 41736\n",
      "Sharpe: 0.847\n",
      "=================================\n",
      "======ddpg Validation from:  2019-01-04 to  2019-04-05\n",
      "ddpg Sharpe Ratio:  0.35099788799683435\n",
      "======td3 Training========\n",
      "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.0001}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/td3/td3_945_1\n",
      "day: 2517, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4838951.07\n",
      "total_reward: 3838951.07\n",
      "total_cost: 1561.45\n",
      "total_trades: 35444\n",
      "Sharpe: 1.038\n",
      "=================================\n",
      "======td3 Validation from:  2019-01-04 to  2019-04-05\n",
      "td3 Sharpe Ratio:  0.2837281969747225\n",
      "======sac Training========\n",
      "{'batch_size': 64, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/sac/sac_945_1\n",
      "day: 2517, episode: 15\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2348175.92\n",
      "total_reward: 1348175.92\n",
      "total_cost: 42494.80\n",
      "total_trades: 42521\n",
      "Sharpe: 0.496\n",
      "=================================\n",
      "======sac Validation from:  2019-01-04 to  2019-04-05\n",
      "sac Sharpe Ratio:  0.19226248247183336\n",
      "======ppo Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_945_1\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 123        |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 16         |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.22780924 |\n",
      "-----------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 121         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 33          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018778399 |\n",
      "|    clip_fraction        | 0.233       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | 0.00629     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 5.56        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0266     |\n",
      "|    reward               | -1.0449162  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 16.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 120         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 50          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017123675 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | -0.025      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 14.3        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0213     |\n",
      "|    reward               | 0.44309235  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 27.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 120         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 68          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014703901 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.4       |\n",
      "|    explained_variance   | -0.00191    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 10.8        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0167     |\n",
      "|    reward               | 0.77968794  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 30.2        |\n",
      "-----------------------------------------\n",
      "day: 2517, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3341793.25\n",
      "total_reward: 2341793.25\n",
      "total_cost: 245596.99\n",
      "total_trades: 69937\n",
      "Sharpe: 0.968\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 119         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 85          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.01253584  |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.4       |\n",
      "|    explained_variance   | -0.00526    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 12.8        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0173     |\n",
      "|    reward               | -0.14123164 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 36.8        |\n",
      "-----------------------------------------\n",
      "======ppo Validation from:  2019-01-04 to  2019-04-05\n",
      "ppo Sharpe Ratio:  0.3981218232328919\n",
      "======Best Model Retraining from:  2009-01-01 to  2019-04-05\n",
      "======Trading from:  2019-04-05 to  2019-07-08\n",
      "============================================\n",
      "turbulence_threshold:  160.3155295025199\n",
      "======Model training from:  2009-01-01 to  2019-04-05\n",
      "======a2c Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_1008_1\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 108       |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0.0856    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | 22.5      |\n",
      "|    reward             | 0.6107237 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 0.954     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 108        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 9          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -35.4      |\n",
      "|    reward             | -3.0104985 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.09       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 108       |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0.058     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -147      |\n",
      "|    reward             | 3.8949032 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 13.5      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 108       |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 18        |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -43       |\n",
      "|    reward             | 1.6447773 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 3.69      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 108       |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 23        |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | 413       |\n",
      "|    reward             | -7.245157 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 142       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 108       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 27        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | -0.362    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -92.2     |\n",
      "|    reward             | 2.7715287 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 6.27      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 108       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 32        |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | 0.189     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 27.9      |\n",
      "|    reward             | 1.0656877 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 0.578     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 108        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 36         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | -1.49      |\n",
      "|    reward             | 0.33576685 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 0.397      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 108        |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 41         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0.0057     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | 14.5       |\n",
      "|    reward             | 0.11744565 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 0.853      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 108       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 46        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 238       |\n",
      "|    reward             | -1.655955 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 31.8      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 108       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 50        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -122      |\n",
      "|    reward             | -0.960806 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 9.51      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 108       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 55        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -33.9     |\n",
      "|    reward             | 0.2978745 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.23      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 108        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 59         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | -0.0944    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | -44.1      |\n",
      "|    reward             | -0.5915436 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.85       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 108         |\n",
      "|    iterations         | 1400        |\n",
      "|    time_elapsed       | 64          |\n",
      "|    total_timesteps    | 7000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.2       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1399        |\n",
      "|    policy_loss        | 25.6        |\n",
      "|    reward             | -0.11532672 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 1.01        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 108       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 69        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -36.7     |\n",
      "|    reward             | -0.984841 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 4.85      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 108        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 73         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | 67.2       |\n",
      "|    reward             | -0.5566893 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.72       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 108        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 78         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0.0121     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | 60.6       |\n",
      "|    reward             | 0.28073972 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 5.77       |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 108          |\n",
      "|    iterations         | 1800         |\n",
      "|    time_elapsed       | 82           |\n",
      "|    total_timesteps    | 9000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -41.3        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1799         |\n",
      "|    policy_loss        | 65.3         |\n",
      "|    reward             | -0.037963193 |\n",
      "|    std                | 1.01         |\n",
      "|    value_loss         | 4.85         |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 108        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 87         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | 99.7       |\n",
      "|    reward             | -0.5283917 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 12.4       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 108        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 91         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | -36.5      |\n",
      "|    reward             | -1.2419379 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.45       |\n",
      "--------------------------------------\n",
      "======a2c Validation from:  2019-04-05 to  2019-07-08\n",
      "a2c Sharpe Ratio:  0.10131358273953452\n",
      "======ddpg Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_1008_1\n",
      "day: 2580, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5614820.53\n",
      "total_reward: 4614820.53\n",
      "total_cost: 4310.61\n",
      "total_trades: 39734\n",
      "Sharpe: 1.224\n",
      "=================================\n",
      "======ddpg Validation from:  2019-04-05 to  2019-07-08\n",
      "ddpg Sharpe Ratio:  0.12491111812980042\n",
      "======td3 Training========\n",
      "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.0001}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/td3/td3_1008_1\n",
      "day: 2580, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5600889.72\n",
      "total_reward: 4600889.72\n",
      "total_cost: 1575.23\n",
      "total_trades: 46512\n",
      "Sharpe: 1.172\n",
      "=================================\n",
      "======td3 Validation from:  2019-04-05 to  2019-07-08\n",
      "td3 Sharpe Ratio:  0.14946736934057656\n",
      "======sac Training========\n",
      "{'batch_size': 64, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/sac/sac_1008_1\n",
      "day: 2580, episode: 15\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3026050.70\n",
      "total_reward: 2026050.70\n",
      "total_cost: 36763.62\n",
      "total_trades: 39152\n",
      "Sharpe: 0.783\n",
      "=================================\n",
      "======sac Validation from:  2019-04-05 to  2019-07-08\n",
      "sac Sharpe Ratio:  0.2947395264561564\n",
      "======ppo Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_1008_1\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 121         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 16          |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | -0.23456968 |\n",
      "------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 118         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 34          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021185312 |\n",
      "|    clip_fraction        | 0.236       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | -0.00205    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 7.2         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0221     |\n",
      "|    reward               | -2.6289604  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 13.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 117         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 52          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029696152 |\n",
      "|    clip_fraction        | 0.283       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | 0.0108      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 7.19        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0204     |\n",
      "|    reward               | -1.5664634  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 17.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 117         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 69          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017807646 |\n",
      "|    clip_fraction        | 0.211       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | -0.00331    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 11.9        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0158     |\n",
      "|    reward               | -0.27065137 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 30.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 117         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 87          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015159352 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | 0.0163      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 11.2        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0187     |\n",
      "|    reward               | 3.3203044   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 27.3        |\n",
      "-----------------------------------------\n",
      "======ppo Validation from:  2019-04-05 to  2019-07-08\n",
      "ppo Sharpe Ratio:  0.1696912557371619\n",
      "======Best Model Retraining from:  2009-01-01 to  2019-07-08\n",
      "======Trading from:  2019-07-08 to  2019-10-04\n",
      "============================================\n",
      "turbulence_threshold:  160.3155295025199\n",
      "======Model training from:  2009-01-01 to  2019-07-08\n",
      "======a2c Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_1071_1\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 106        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | -1.38      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -80.8      |\n",
      "|    reward             | 0.12136322 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 3.71       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 106         |\n",
      "|    iterations         | 200         |\n",
      "|    time_elapsed       | 9           |\n",
      "|    total_timesteps    | 1000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.3       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 199         |\n",
      "|    policy_loss        | -59.7       |\n",
      "|    reward             | -0.25323385 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 2.86        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 107       |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -289      |\n",
      "|    reward             | 4.8320684 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 62.7      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 107      |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 18       |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.4    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -62.5    |\n",
      "|    reward             | 0.747681 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 9.13     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 107        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 23         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 364        |\n",
      "|    reward             | -6.6092787 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 103        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 107         |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 27          |\n",
      "|    total_timesteps    | 3000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.4       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | -112        |\n",
      "|    reward             | -0.01189167 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 10          |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 107        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 32         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -146       |\n",
      "|    reward             | -1.0757703 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 14.9       |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 107          |\n",
      "|    iterations         | 800          |\n",
      "|    time_elapsed       | 37           |\n",
      "|    total_timesteps    | 4000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -41.3        |\n",
      "|    explained_variance | 1.19e-07     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 799          |\n",
      "|    policy_loss        | -44.5        |\n",
      "|    reward             | -0.055121437 |\n",
      "|    std                | 1            |\n",
      "|    value_loss         | 3.3          |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 107        |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 41         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0.0282     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | -143       |\n",
      "|    reward             | -3.4780369 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 18.4       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 107        |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 46         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | 125        |\n",
      "|    reward             | -0.5441914 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 59.8       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 107        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 50         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0.0969     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | -206       |\n",
      "|    reward             | -0.5624465 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 24.3       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 107         |\n",
      "|    iterations         | 1200        |\n",
      "|    time_elapsed       | 55          |\n",
      "|    total_timesteps    | 6000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.3       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1199        |\n",
      "|    policy_loss        | 213         |\n",
      "|    reward             | -0.41876203 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 31.2        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 107        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 60         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0.0567     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 17.7       |\n",
      "|    reward             | 0.91245687 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 2.04       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 107        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 64         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | -104       |\n",
      "|    reward             | -1.8244697 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 5.86       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 107       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 69        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -257      |\n",
      "|    reward             | 2.7615702 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 42.6      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 107       |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 74        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | 218       |\n",
      "|    reward             | 2.0476913 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 37.1      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 107         |\n",
      "|    iterations         | 1700        |\n",
      "|    time_elapsed       | 78          |\n",
      "|    total_timesteps    | 8500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.2       |\n",
      "|    explained_variance | 0.274       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1699        |\n",
      "|    policy_loss        | 28.1        |\n",
      "|    reward             | -0.13319792 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 0.718       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 107        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 83         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | -0.296     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | 90.4       |\n",
      "|    reward             | -1.8357159 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 6.72       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 107        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 88         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | 117        |\n",
      "|    reward             | 0.14488491 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 14.4       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 107        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 92         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | 66.5       |\n",
      "|    reward             | 0.72278655 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 3.41       |\n",
      "--------------------------------------\n",
      "======a2c Validation from:  2019-07-08 to  2019-10-04\n",
      "a2c Sharpe Ratio:  -0.2881612823521956\n",
      "======ddpg Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_1071_1\n",
      "day: 2643, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5096823.89\n",
      "total_reward: 4096823.89\n",
      "total_cost: 4725.20\n",
      "total_trades: 53601\n",
      "Sharpe: 1.085\n",
      "=================================\n",
      "======ddpg Validation from:  2019-07-08 to  2019-10-04\n",
      "ddpg Sharpe Ratio:  -0.16191382021943249\n",
      "======td3 Training========\n",
      "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.0001}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/td3/td3_1071_1\n",
      "day: 2643, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6111297.29\n",
      "total_reward: 5111297.29\n",
      "total_cost: 1319.01\n",
      "total_trades: 37103\n",
      "Sharpe: 1.149\n",
      "=================================\n",
      "======td3 Validation from:  2019-07-08 to  2019-10-04\n",
      "td3 Sharpe Ratio:  -0.10224427828344662\n",
      "======sac Training========\n",
      "{'batch_size': 64, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/sac/sac_1071_1\n",
      "day: 2643, episode: 15\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5443906.69\n",
      "total_reward: 4443906.69\n",
      "total_cost: 4587.10\n",
      "total_trades: 38484\n",
      "Sharpe: 1.143\n",
      "=================================\n",
      "======sac Validation from:  2019-07-08 to  2019-10-04\n",
      "sac Sharpe Ratio:  -0.12187597449250602\n",
      "======ppo Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_1071_1\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 119        |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 17         |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.37524062 |\n",
      "-----------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 116        |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 35         |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01606134 |\n",
      "|    clip_fraction        | 0.19       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -41.2      |\n",
      "|    explained_variance   | 0.00153    |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 8.6        |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.0222    |\n",
      "|    reward               | -4.184833  |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 14.8       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 115         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 53          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.01705346  |\n",
      "|    clip_fraction        | 0.223       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | -0.000837   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 9.61        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0228     |\n",
      "|    reward               | -0.60932267 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 20          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 115         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 70          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012424273 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | 0.0181      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 9.9         |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0177     |\n",
      "|    reward               | 0.0801491   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 24.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 115         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 88          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014179115 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | -0.0122     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 14.7        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0159     |\n",
      "|    reward               | -1.2346894  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 36.2        |\n",
      "-----------------------------------------\n",
      "======ppo Validation from:  2019-07-08 to  2019-10-04\n",
      "ppo Sharpe Ratio:  -0.23754405646316473\n",
      "======Best Model Retraining from:  2009-01-01 to  2019-10-04\n",
      "======Trading from:  2019-10-04 to  2020-01-06\n",
      "============================================\n",
      "turbulence_threshold:  160.3155295025199\n",
      "======Model training from:  2009-01-01 to  2019-10-04\n",
      "======a2c Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_1134_1\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 106       |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -17.1     |\n",
      "|    reward             | 0.5251774 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 0.366     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 105        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 9          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -55.1      |\n",
      "|    reward             | -1.9356234 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.22       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 105       |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | -0.0456   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -241      |\n",
      "|    reward             | 3.6344824 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 40.3      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 105        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 18         |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0.0356     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -10.3      |\n",
      "|    reward             | -0.7790293 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 3.35       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 105        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 23         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 353        |\n",
      "|    reward             | -7.9338713 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 135        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 105       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 28        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | 13.7      |\n",
      "|    reward             | 0.5915326 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 0.486     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 105        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 33         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | 82         |\n",
      "|    reward             | 0.08270966 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 4.3        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 105        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 37         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | 0.0997     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | 142        |\n",
      "|    reward             | -0.3932235 |\n",
      "|    std                | 0.997      |\n",
      "|    value_loss         | 24.1       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 105        |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 42         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | -95.2      |\n",
      "|    reward             | -3.1051655 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 40.3       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 105       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 47        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -1.18e+03 |\n",
      "|    reward             | 4.915243  |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 728       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 105        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 52         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | 66.4       |\n",
      "|    reward             | -1.3530899 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 3.5        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 105        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 56         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | 140        |\n",
      "|    reward             | 0.37939814 |\n",
      "|    std                | 0.997      |\n",
      "|    value_loss         | 19.1       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 105       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 61        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | 154       |\n",
      "|    reward             | 0.6494191 |\n",
      "|    std                | 0.996     |\n",
      "|    value_loss         | 20.2      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 105       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 66        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 195       |\n",
      "|    reward             | -0.696987 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 25.6      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 105       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 70        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -56.1     |\n",
      "|    reward             | -1.359642 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 3.29      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 105       |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 75        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -88.6     |\n",
      "|    reward             | 3.9856663 |\n",
      "|    std                | 0.999     |\n",
      "|    value_loss         | 22.9      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 105      |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 80       |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.1    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | -193     |\n",
      "|    reward             | 4.491699 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 25.2     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 105         |\n",
      "|    iterations         | 1800        |\n",
      "|    time_elapsed       | 84          |\n",
      "|    total_timesteps    | 9000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.1       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1799        |\n",
      "|    policy_loss        | -151        |\n",
      "|    reward             | -0.68172944 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 13.6        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 105        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 89         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | 94.4       |\n",
      "|    reward             | -0.5831334 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 7.33       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 105         |\n",
      "|    iterations         | 2000        |\n",
      "|    time_elapsed       | 94          |\n",
      "|    total_timesteps    | 10000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1999        |\n",
      "|    policy_loss        | 48.9        |\n",
      "|    reward             | 0.036051057 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 3.02        |\n",
      "---------------------------------------\n",
      "======a2c Validation from:  2019-10-04 to  2020-01-06\n",
      "a2c Sharpe Ratio:  0.31413572827855657\n",
      "======ddpg Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_1134_1\n",
      "day: 2706, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6202498.47\n",
      "total_reward: 5202498.47\n",
      "total_cost: 4943.26\n",
      "total_trades: 39275\n",
      "Sharpe: 1.296\n",
      "=================================\n",
      "======ddpg Validation from:  2019-10-04 to  2020-01-06\n",
      "ddpg Sharpe Ratio:  0.48553323575643026\n",
      "======td3 Training========\n",
      "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.0001}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/td3/td3_1134_1\n",
      "day: 2706, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4873645.64\n",
      "total_reward: 3873645.64\n",
      "total_cost: 1211.77\n",
      "total_trades: 39719\n",
      "Sharpe: 1.010\n",
      "=================================\n",
      "======td3 Validation from:  2019-10-04 to  2020-01-06\n",
      "td3 Sharpe Ratio:  0.4026566515207464\n",
      "======sac Training========\n",
      "{'batch_size': 64, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/sac/sac_1134_1\n",
      "day: 2706, episode: 15\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6284723.47\n",
      "total_reward: 5284723.47\n",
      "total_cost: 24201.74\n",
      "total_trades: 42823\n",
      "Sharpe: 1.140\n",
      "=================================\n",
      "======sac Validation from:  2019-10-04 to  2020-01-06\n",
      "sac Sharpe Ratio:  0.5278987889224299\n",
      "======ppo Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_1134_1\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    fps             | 117       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 17        |\n",
      "|    total_timesteps | 2048      |\n",
      "| train/             |           |\n",
      "|    reward          | 0.2901219 |\n",
      "----------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 114        |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 35         |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01451285 |\n",
      "|    clip_fraction        | 0.2        |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -41.2      |\n",
      "|    explained_variance   | 0.00191    |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 5.07       |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.021     |\n",
      "|    reward               | 0.26614067 |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 13.3       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 114         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 53          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015469183 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | 0.00615     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 7.35        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0248     |\n",
      "|    reward               | -2.0784695  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 16.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 113         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 72          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022630427 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | -0.0271     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 6.19        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0218     |\n",
      "|    reward               | 0.28536838  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 14.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 113         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 90          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020708203 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | -0.0137     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 20.3        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.019      |\n",
      "|    reward               | 0.5691286   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 40.4        |\n",
      "-----------------------------------------\n",
      "======ppo Validation from:  2019-10-04 to  2020-01-06\n",
      "ppo Sharpe Ratio:  0.4108882046989847\n",
      "======Best Model Retraining from:  2009-01-01 to  2020-01-06\n",
      "======Trading from:  2020-01-06 to  2020-04-06\n",
      "Ensemble Strategy took:  163.73040033578872  minutes\n"
     ]
    }
   ],
   "source": [
    "df_summary = ensemble_agent.run_ensemble_strategy(\n",
    "    A2C_model_kwargs,\n",
    "    PPO_model_kwargs,\n",
    "    DDPG_model_kwargs,\n",
    "    SAC_model_kwargs,\n",
    "    TD3_model_kwargs,\n",
    "    timesteps_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T16:11:06.013311Z",
     "iopub.status.busy": "2025-05-09T16:11:06.012985Z",
     "iopub.status.idle": "2025-05-09T16:11:06.024506Z",
     "shell.execute_reply": "2025-05-09T16:11:06.023848Z",
     "shell.execute_reply.started": "2025-05-09T16:11:06.013290Z"
    },
    "id": "-0qd8acMtj1f",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iter</th>\n",
       "      <th>Val Start</th>\n",
       "      <th>Val End</th>\n",
       "      <th>Model Used</th>\n",
       "      <th>A2C Sharpe</th>\n",
       "      <th>PPO Sharpe</th>\n",
       "      <th>DDPG Sharpe</th>\n",
       "      <th>SAC Sharpe</th>\n",
       "      <th>TD3 Sharpe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>126</td>\n",
       "      <td>2015-10-02</td>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>TD3</td>\n",
       "      <td>-0.042865</td>\n",
       "      <td>-0.068222</td>\n",
       "      <td>-0.015456</td>\n",
       "      <td>-0.039202</td>\n",
       "      <td>0.004611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>189</td>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>2016-04-05</td>\n",
       "      <td>A2C</td>\n",
       "      <td>0.310542</td>\n",
       "      <td>0.167452</td>\n",
       "      <td>0.163125</td>\n",
       "      <td>0.197997</td>\n",
       "      <td>0.197801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>252</td>\n",
       "      <td>2016-04-05</td>\n",
       "      <td>2016-07-05</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>0.047644</td>\n",
       "      <td>0.023558</td>\n",
       "      <td>0.096198</td>\n",
       "      <td>-0.046449</td>\n",
       "      <td>0.032208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>315</td>\n",
       "      <td>2016-07-05</td>\n",
       "      <td>2016-10-03</td>\n",
       "      <td>A2C</td>\n",
       "      <td>0.117029</td>\n",
       "      <td>-0.08554</td>\n",
       "      <td>0.036067</td>\n",
       "      <td>0.090584</td>\n",
       "      <td>-0.034953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>378</td>\n",
       "      <td>2016-10-03</td>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>0.416617</td>\n",
       "      <td>0.11444</td>\n",
       "      <td>0.600698</td>\n",
       "      <td>0.36674</td>\n",
       "      <td>0.150965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>441</td>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>2017-04-04</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>0.055696</td>\n",
       "      <td>-0.054737</td>\n",
       "      <td>0.348465</td>\n",
       "      <td>0.305002</td>\n",
       "      <td>0.303212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>504</td>\n",
       "      <td>2017-04-04</td>\n",
       "      <td>2017-07-05</td>\n",
       "      <td>TD3</td>\n",
       "      <td>-0.159207</td>\n",
       "      <td>0.095912</td>\n",
       "      <td>0.11883</td>\n",
       "      <td>0.142841</td>\n",
       "      <td>0.148706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>567</td>\n",
       "      <td>2017-07-05</td>\n",
       "      <td>2017-10-03</td>\n",
       "      <td>SAC</td>\n",
       "      <td>0.224512</td>\n",
       "      <td>0.309304</td>\n",
       "      <td>0.371815</td>\n",
       "      <td>0.388928</td>\n",
       "      <td>0.227055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>630</td>\n",
       "      <td>2017-10-03</td>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>SAC</td>\n",
       "      <td>0.571926</td>\n",
       "      <td>0.469694</td>\n",
       "      <td>0.345554</td>\n",
       "      <td>0.737943</td>\n",
       "      <td>0.647932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>693</td>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>2018-04-05</td>\n",
       "      <td>SAC</td>\n",
       "      <td>-0.068268</td>\n",
       "      <td>-0.22136</td>\n",
       "      <td>-0.064492</td>\n",
       "      <td>-0.05968</td>\n",
       "      <td>-0.08602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>756</td>\n",
       "      <td>2018-04-05</td>\n",
       "      <td>2018-07-05</td>\n",
       "      <td>TD3</td>\n",
       "      <td>0.154824</td>\n",
       "      <td>-0.086112</td>\n",
       "      <td>0.004537</td>\n",
       "      <td>-0.066631</td>\n",
       "      <td>0.179218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>819</td>\n",
       "      <td>2018-07-05</td>\n",
       "      <td>2018-10-03</td>\n",
       "      <td>TD3</td>\n",
       "      <td>0.412529</td>\n",
       "      <td>0.391442</td>\n",
       "      <td>0.486038</td>\n",
       "      <td>0.541775</td>\n",
       "      <td>0.626946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>882</td>\n",
       "      <td>2018-10-03</td>\n",
       "      <td>2019-01-04</td>\n",
       "      <td>TD3</td>\n",
       "      <td>-0.111113</td>\n",
       "      <td>-0.298138</td>\n",
       "      <td>-0.162481</td>\n",
       "      <td>-0.113747</td>\n",
       "      <td>-0.091134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>945</td>\n",
       "      <td>2019-01-04</td>\n",
       "      <td>2019-04-05</td>\n",
       "      <td>PPO</td>\n",
       "      <td>0.311463</td>\n",
       "      <td>0.398122</td>\n",
       "      <td>0.350998</td>\n",
       "      <td>0.192262</td>\n",
       "      <td>0.283728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1008</td>\n",
       "      <td>2019-04-05</td>\n",
       "      <td>2019-07-08</td>\n",
       "      <td>SAC</td>\n",
       "      <td>0.101314</td>\n",
       "      <td>0.169691</td>\n",
       "      <td>0.124911</td>\n",
       "      <td>0.29474</td>\n",
       "      <td>0.149467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1071</td>\n",
       "      <td>2019-07-08</td>\n",
       "      <td>2019-10-04</td>\n",
       "      <td>TD3</td>\n",
       "      <td>-0.288161</td>\n",
       "      <td>-0.237544</td>\n",
       "      <td>-0.161914</td>\n",
       "      <td>-0.121876</td>\n",
       "      <td>-0.102244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1134</td>\n",
       "      <td>2019-10-04</td>\n",
       "      <td>2020-01-06</td>\n",
       "      <td>SAC</td>\n",
       "      <td>0.314136</td>\n",
       "      <td>0.410888</td>\n",
       "      <td>0.485533</td>\n",
       "      <td>0.527899</td>\n",
       "      <td>0.402657</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Iter   Val Start     Val End Model Used A2C Sharpe PPO Sharpe DDPG Sharpe  \\\n",
       "0    126  2015-10-02  2016-01-04        TD3  -0.042865  -0.068222   -0.015456   \n",
       "1    189  2016-01-04  2016-04-05        A2C   0.310542   0.167452    0.163125   \n",
       "2    252  2016-04-05  2016-07-05       DDPG   0.047644   0.023558    0.096198   \n",
       "3    315  2016-07-05  2016-10-03        A2C   0.117029   -0.08554    0.036067   \n",
       "4    378  2016-10-03  2017-01-03       DDPG   0.416617    0.11444    0.600698   \n",
       "5    441  2017-01-03  2017-04-04       DDPG   0.055696  -0.054737    0.348465   \n",
       "6    504  2017-04-04  2017-07-05        TD3  -0.159207   0.095912     0.11883   \n",
       "7    567  2017-07-05  2017-10-03        SAC   0.224512   0.309304    0.371815   \n",
       "8    630  2017-10-03  2018-01-03        SAC   0.571926   0.469694    0.345554   \n",
       "9    693  2018-01-03  2018-04-05        SAC  -0.068268   -0.22136   -0.064492   \n",
       "10   756  2018-04-05  2018-07-05        TD3   0.154824  -0.086112    0.004537   \n",
       "11   819  2018-07-05  2018-10-03        TD3   0.412529   0.391442    0.486038   \n",
       "12   882  2018-10-03  2019-01-04        TD3  -0.111113  -0.298138   -0.162481   \n",
       "13   945  2019-01-04  2019-04-05        PPO   0.311463   0.398122    0.350998   \n",
       "14  1008  2019-04-05  2019-07-08        SAC   0.101314   0.169691    0.124911   \n",
       "15  1071  2019-07-08  2019-10-04        TD3  -0.288161  -0.237544   -0.161914   \n",
       "16  1134  2019-10-04  2020-01-06        SAC   0.314136   0.410888    0.485533   \n",
       "\n",
       "   SAC Sharpe TD3 Sharpe  \n",
       "0   -0.039202   0.004611  \n",
       "1    0.197997   0.197801  \n",
       "2   -0.046449   0.032208  \n",
       "3    0.090584  -0.034953  \n",
       "4     0.36674   0.150965  \n",
       "5    0.305002   0.303212  \n",
       "6    0.142841   0.148706  \n",
       "7    0.388928   0.227055  \n",
       "8    0.737943   0.647932  \n",
       "9    -0.05968   -0.08602  \n",
       "10  -0.066631   0.179218  \n",
       "11   0.541775   0.626946  \n",
       "12  -0.113747  -0.091134  \n",
       "13   0.192262   0.283728  \n",
       "14    0.29474   0.149467  \n",
       "15  -0.121876  -0.102244  \n",
       "16   0.527899   0.402657  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6vvNSC6h1jZ"
   },
   "source": [
    "<a id='6'></a>\n",
    "# Part 7: Backtest Our Strategy\n",
    "Backtesting plays a key role in evaluating the performance of a trading strategy. Automated backtesting tool is preferred because it reduces the human error. We usually use the Quantopian pyfolio package to backtest our trading strategies. It is easy to use and consists of various individual plots that provide a comprehensive image of the performance of a trading strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T16:11:06.025816Z",
     "iopub.status.busy": "2025-05-09T16:11:06.025545Z",
     "iopub.status.idle": "2025-05-09T16:11:06.040980Z",
     "shell.execute_reply": "2025-05-09T16:11:06.040408Z",
     "shell.execute_reply.started": "2025-05-09T16:11:06.025798Z"
    },
    "id": "X4JKB--8tj1g"
   },
   "outputs": [],
   "source": [
    "unique_trade_date = processed[(processed.date > TEST_START_DATE)&(processed.date <= TEST_END_DATE)].date.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-09T16:18:28.844715Z",
     "iopub.status.busy": "2025-05-09T16:18:28.844281Z",
     "iopub.status.idle": "2025-05-09T16:18:28.878294Z",
     "shell.execute_reply": "2025-05-09T16:18:28.877536Z",
     "shell.execute_reply.started": "2025-05-09T16:18:28.844686Z"
    },
    "id": "q9mKF7GGtj1g",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sharpe Ratio:  0.80767641847303\n"
     ]
    }
   ],
   "source": [
    "df_trade_date = pd.DataFrame({'datadate':unique_trade_date})\n",
    "\n",
    "df_account_value=pd.DataFrame()\n",
    "for i in range(rebalance_window+validation_window, len(unique_trade_date)+1,rebalance_window):\n",
    "    temp = pd.read_csv('results/account_value_trade_{}_{}.csv'.format('ensemble',i))\n",
    "    df_account_value = pd.concat([df_account_value,temp], ignore_index=True)\n",
    "sharpe=(252**0.5)*df_account_value.account_value.pct_change(1).mean()/df_account_value.account_value.pct_change(1).std()\n",
    "print('Sharpe Ratio: ',sharpe)\n",
    "df_account_value=df_account_value.join(df_trade_date[validation_window:].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T16:18:31.988422Z",
     "iopub.status.busy": "2025-05-09T16:18:31.987980Z",
     "iopub.status.idle": "2025-05-09T16:18:31.996969Z",
     "shell.execute_reply": "2025-05-09T16:18:31.996305Z",
     "shell.execute_reply.started": "2025-05-09T16:18:31.988395Z"
    },
    "id": "oyosyW7_tj1g",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>account_value</th>\n",
       "      <th>date</th>\n",
       "      <th>daily_return</th>\n",
       "      <th>datadate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>999871.664010</td>\n",
       "      <td>2016-01-05</td>\n",
       "      <td>-0.000128</td>\n",
       "      <td>2016-01-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>996499.081928</td>\n",
       "      <td>2016-01-06</td>\n",
       "      <td>-0.003373</td>\n",
       "      <td>2016-01-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>987475.302824</td>\n",
       "      <td>2016-01-07</td>\n",
       "      <td>-0.009055</td>\n",
       "      <td>2016-01-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>982613.960993</td>\n",
       "      <td>2016-01-08</td>\n",
       "      <td>-0.004923</td>\n",
       "      <td>2016-01-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    account_value        date  daily_return    datadate\n",
       "0  1000000.000000  2016-01-04           NaN  2016-01-04\n",
       "1   999871.664010  2016-01-05     -0.000128  2016-01-05\n",
       "2   996499.081928  2016-01-06     -0.003373  2016-01-06\n",
       "3   987475.302824  2016-01-07     -0.009055  2016-01-07\n",
       "4   982613.960993  2016-01-08     -0.004923  2016-01-08"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_account_value.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T16:18:34.048802Z",
     "iopub.status.busy": "2025-05-09T16:18:34.048392Z",
     "iopub.status.idle": "2025-05-09T16:18:35.471180Z",
     "shell.execute_reply": "2025-05-09T16:18:35.470509Z",
     "shell.execute_reply.started": "2025-05-09T16:18:34.048775Z"
    },
    "id": "wLsRdw2Ctj1h",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "df_account_value.account_value.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lr2zX7ZxNyFQ"
   },
   "source": [
    "<a id='6.1'></a>\n",
    "## 7.1 BackTestStats\n",
    "pass in df_account_value, this information is stored in env class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T16:18:39.489737Z",
     "iopub.status.busy": "2025-05-09T16:18:39.489307Z",
     "iopub.status.idle": "2025-05-09T16:18:39.509685Z",
     "shell.execute_reply": "2025-05-09T16:18:39.509036Z",
     "shell.execute_reply.started": "2025-05-09T16:18:39.489709Z"
    },
    "id": "Nzkr9yv-AdV_",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============Get Backtest Results===========\n",
      "Annual return          0.096180\n",
      "Cumulative returns     0.477402\n",
      "Annual volatility      0.123250\n",
      "Sharpe ratio           0.807676\n",
      "Calmar ratio           0.719353\n",
      "Stability              0.954629\n",
      "Max drawdown          -0.133704\n",
      "Omega ratio            1.166678\n",
      "Sortino ratio          1.118417\n",
      "Skew                        NaN\n",
      "Kurtosis                    NaN\n",
      "Tail ratio             1.080778\n",
      "Daily value at risk   -0.015133\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"==============Get Backtest Results===========\")\n",
    "now = datetime.datetime.now().strftime('%Y%m%d-%Hh%M')\n",
    "\n",
    "perf_stats_all = backtest_stats(account_value=df_account_value)\n",
    "perf_stats_all = pd.DataFrame(perf_stats_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T16:38:09.897323Z",
     "iopub.status.busy": "2025-05-09T16:38:09.896862Z",
     "iopub.status.idle": "2025-05-09T16:38:09.902078Z",
     "shell.execute_reply": "2025-05-09T16:38:09.901440Z",
     "shell.execute_reply.started": "2025-05-09T16:38:09.897296Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2016-01-04'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_account_value.loc[0,'date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-05-09T16:37:41.122767Z",
     "iopub.status.busy": "2025-05-09T16:37:41.122341Z",
     "iopub.status.idle": "2025-05-09T16:37:41.127590Z",
     "shell.execute_reply": "2025-05-09T16:37:41.126964Z",
     "shell.execute_reply.started": "2025-05-09T16:37:41.122741Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2020-04-03'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_account_value.loc[len(df_account_value)-1,'date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-09T16:47:06.072744Z",
     "iopub.status.busy": "2025-05-09T16:47:06.072319Z",
     "iopub.status.idle": "2025-05-09T16:47:06.094170Z",
     "shell.execute_reply": "2025-05-09T16:47:06.093469Z",
     "shell.execute_reply.started": "2025-05-09T16:47:06.072718Z"
    },
    "id": "DiHhM1YkoCel",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============Get Baseline Stats===========\n",
      "Annual return          0.053696\n",
      "Cumulative returns     0.248674\n",
      "Annual volatility      0.194950\n",
      "Sharpe ratio           0.366925\n",
      "Calmar ratio           0.144786\n",
      "Stability              0.834802\n",
      "Max drawdown          -0.370862\n",
      "Omega ratio            1.088185\n",
      "Sortino ratio          0.491094\n",
      "Skew                        NaN\n",
      "Kurtosis                    NaN\n",
      "Tail ratio             0.880577\n",
      "Daily value at risk   -0.024277\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#baseline stats\n",
    "print(\"==============Get Baseline Stats===========\")\n",
    "#df_dji_ = get_baseline(\n",
    "#        ticker=\"^DJI\",\n",
    "#        start = df_account_value.loc[0,'date'],\n",
    "#        end = df_account_value.loc[len(df_account_value)-1,'date'])\n",
    "\n",
    "df_dji_ = pd.read_csv('dji.csv')\n",
    "\n",
    "stats = backtest_stats(df_dji_, value_col_name = 'close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T16:47:08.878139Z",
     "iopub.status.busy": "2025-05-09T16:47:08.877683Z",
     "iopub.status.idle": "2025-05-09T16:47:08.902047Z",
     "shell.execute_reply": "2025-05-09T16:47:08.901284Z",
     "shell.execute_reply.started": "2025-05-09T16:47:08.878112Z"
    },
    "id": "RhJ9whD75WTs",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_dji:              date           dji\n",
      "0     2016-01-04  1.000000e+06\n",
      "1     2016-01-05  1.000567e+06\n",
      "2     2016-01-06  9.858633e+05\n",
      "3     2016-01-07  9.629808e+05\n",
      "4     2016-01-08  9.532047e+05\n",
      "...          ...           ...\n",
      "1066  2020-03-30  1.301974e+06\n",
      "1067  2020-03-31  1.278048e+06\n",
      "1068  2020-04-01  1.221271e+06\n",
      "1069  2020-04-02  1.248674e+06\n",
      "1070  2020-04-03           NaN\n",
      "\n",
      "[1071 rows x 2 columns]\n",
      "df_dji:                       dji\n",
      "date                    \n",
      "2016-01-04  1.000000e+06\n",
      "2016-01-05  1.000567e+06\n",
      "2016-01-06  9.858633e+05\n",
      "2016-01-07  9.629808e+05\n",
      "2016-01-08  9.532047e+05\n",
      "...                  ...\n",
      "2020-03-30  1.301974e+06\n",
      "2020-03-31  1.278048e+06\n",
      "2020-04-01  1.221271e+06\n",
      "2020-04-02  1.248674e+06\n",
      "2020-04-03           NaN\n",
      "\n",
      "[1071 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "df_dji = pd.DataFrame()\n",
    "df_dji['date'] = df_account_value['date']\n",
    "df_dji['dji'] = df_dji_['close'] / df_dji_['close'][0] * env_kwargs[\"initial_amount\"]\n",
    "print(\"df_dji: \", df_dji)\n",
    "df_dji.to_csv(\"df_dji.csv\")\n",
    "df_dji = df_dji.set_index(df_dji.columns[0])\n",
    "print(\"df_dji: \", df_dji)\n",
    "df_dji.to_csv(\"df_dji+.csv\")\n",
    "\n",
    "df_account_value.to_csv('df_account_value.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9U6Suru3h1jc"
   },
   "source": [
    "<a id='6.2'></a>\n",
    "## 7.2 BackTestPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T16:47:13.245092Z",
     "iopub.status.busy": "2025-05-09T16:47:13.244656Z",
     "iopub.status.idle": "2025-05-09T16:47:14.070497Z",
     "shell.execute_reply": "2025-05-09T16:47:14.069809Z",
     "shell.execute_reply.started": "2025-05-09T16:47:13.245065Z"
    },
    "id": "HggausPRoCem",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_result_ensemble.columns:  Index(['ensemble'], dtype='object')\n",
      "df_trade_date:          datadate\n",
      "0     2015-10-02\n",
      "1     2015-10-05\n",
      "2     2015-10-06\n",
      "3     2015-10-07\n",
      "4     2015-10-08\n",
      "...          ...\n",
      "1152  2020-05-01\n",
      "1153  2020-05-04\n",
      "1154  2020-05-05\n",
      "1155  2020-05-06\n",
      "1156  2020-05-07\n",
      "\n",
      "[1157 rows x 1 columns]\n",
      "df_result_ensemble:                  ensemble\n",
      "date                    \n",
      "2016-01-04  1.000000e+06\n",
      "2016-01-05  9.998717e+05\n",
      "2016-01-06  9.964991e+05\n",
      "2016-01-07  9.874753e+05\n",
      "2016-01-08  9.826140e+05\n",
      "...                  ...\n",
      "2020-03-30  1.479217e+06\n",
      "2020-03-31  1.473654e+06\n",
      "2020-04-01  1.458538e+06\n",
      "2020-04-02  1.477933e+06\n",
      "2020-04-03  1.477402e+06\n",
      "\n",
      "[1071 rows x 1 columns]\n",
      "==============Compare to DJIA===========\n",
      "result:                  ensemble           dji\n",
      "date                                  \n",
      "2016-01-04  1.000000e+06  1.000000e+06\n",
      "2016-01-05  9.998717e+05  1.000567e+06\n",
      "2016-01-06  9.964991e+05  9.858633e+05\n",
      "2016-01-07  9.874753e+05  9.629808e+05\n",
      "2016-01-08  9.826140e+05  9.532047e+05\n",
      "...                  ...           ...\n",
      "2020-03-30  1.479217e+06  1.301974e+06\n",
      "2020-03-31  1.473654e+06  1.278048e+06\n",
      "2020-04-01  1.458538e+06  1.221271e+06\n",
      "2020-04-02  1.477933e+06  1.248674e+06\n",
      "2020-04-03  1.477402e+06           NaN\n",
      "\n",
      "[1071 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# print(\"==============Compare to DJIA===========\")\n",
    "# %matplotlib inline\n",
    "# # S&P 500: ^GSPC\n",
    "# # Dow Jones Index: ^DJI\n",
    "# # NASDAQ 100: ^NDX\n",
    "# backtest_plot(df_account_value,\n",
    "#               baseline_ticker = '^DJI',\n",
    "#               baseline_start = df_account_value.loc[0,'date'],\n",
    "#               baseline_end = df_account_value.loc[len(df_account_value)-1,'date'])\n",
    "df.to_csv(\"df.csv\")\n",
    "df_result_ensemble = pd.DataFrame({'date': df_account_value['date'], 'ensemble': df_account_value['account_value']})\n",
    "df_result_ensemble = df_result_ensemble.set_index('date')\n",
    "\n",
    "print(\"df_result_ensemble.columns: \", df_result_ensemble.columns)\n",
    "\n",
    "# df_result_ensemble.drop(df_result_ensemble.columns[0], axis = 1)\n",
    "print(\"df_trade_date: \", df_trade_date)\n",
    "# df_result_ensemble['date'] = df_trade_date['datadate']\n",
    "# df_result_ensemble['account_value'] = df_account_value['account_value']\n",
    "df_result_ensemble.to_csv(\"df_result_ensemble.csv\")\n",
    "print(\"df_result_ensemble: \", df_result_ensemble)\n",
    "print(\"==============Compare to DJIA===========\")\n",
    "result = pd.DataFrame()\n",
    "# result = pd.merge(result, df_result_ensemble, left_index=True, right_index=True)\n",
    "# result = pd.merge(result, df_dji, left_index=True, right_index=True)\n",
    "result = pd.merge(df_result_ensemble, df_dji, left_index=True, right_index=True)\n",
    "print(\"result: \", result)\n",
    "result.to_csv(\"result.csv\")\n",
    "result.columns = ['ensemble', 'dji']\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (15,5)\n",
    "plt.figure();\n",
    "result.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBQx4bVQFi-a"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "FinRL_Ensemble_StockTrading_ICAIF_2020.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
